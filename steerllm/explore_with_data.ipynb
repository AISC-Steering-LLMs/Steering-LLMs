{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific setup\n",
    "\n",
    "# !git clone https://github.com/AISC-Steering-LLMs/Steering-LLMs\n",
    "# !pwd\n",
    "# repo_path = '/content/repository/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# For refactored code\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from data_analyser import DataAnalyzer\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "# For datsaet generation\n",
    "import IPython\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7254/571087763.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\".\", job_name=\"experiment\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Hydra for configuration management\n",
    "GlobalHydra.instance().clear()  # Clear any previous Hydra instance\n",
    "initialize(config_path=\".\", job_name=\"experiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "\n",
    "Skip to experiment runs section if you don't want to generate a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for gpt-4-0125-preview, the maximum number of tokens is 4096\n",
    "# including the prompt and the response\n",
    "# Based on Eleni's latest prompts\n",
    "# Assuming an input prompt of 200 words = 250 tokens\n",
    "# And assuming 50 tokens per generated prompt example we want in the response\n",
    "# We can expect to generate a maximum of (4096-250)/50 = 3846/50 = 76.92\n",
    "# So something like 75 generated examples per prompt is the max we can ask for in one go.\n",
    "# 250 prompt tokens = 200 * 0.01/1000 = $0.0025\n",
    "# 3846 completion tokens = 3846 * 0.03/1000 = $0.11538\n",
    "# So the total cost is $0.11788 per 75 prompts.\n",
    "# If we wanted to generate 1000 prompts, it would cost $1.5718\n",
    "# Please check your prompts work with ChatGPT before generating a large dataset using the API\n",
    "\n",
    "model = \"gpt-4-0125-preview\"\n",
    "prompt_structure_dir = \"pairs_v1\"\n",
    "template_file = \"template_multi.j2\"\n",
    "prompt_context_file = \"honesty.json\"\n",
    "num_examples_per_prompt = \"5\" # Must be a whole number inside quote marks. Max is 75.\n",
    "total_num_examples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SRC_PATH = \"../data/inputs\"\n",
    "DATASET_BUILDER_DIR_PATH = os.path.join(SRC_PATH, \"prompts\", prompt_structure_dir)\n",
    "\n",
    "# Number of interations of the prompt to generate the entire dataset\n",
    "num_iterations = math.ceil(total_num_examples/int(num_examples_per_prompt))\n",
    "\n",
    "# Input directories and files\n",
    "template_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"templates\", template_file)\n",
    "prompt_context, _ = os.path.splitext(prompt_context_file)\n",
    "prompt_context_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"contexts\", prompt_context+\".json\")\n",
    "\n",
    "# Output directories and files\n",
    "dataset_generator_prompt_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"dataset_generator_prompts\", prompt_context+\"_prompt.txt\")\n",
    "generated_dataset_dir = os.path.join(DATASET_BUILDER_DIR_PATH, \"generated_datasets\")\n",
    "generated_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_dataset\")\n",
    "log_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"logs\", prompt_context+\"_log\")\n",
    "combined_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the prompt from the template and template material\n",
    "def render_template_with_data(template_file_path,\n",
    "                              prompt_context_file_path,\n",
    "                              dataset_generator_prompt_file_path,\n",
    "                              num_examples_per_prompt):\n",
    "\n",
    "    # Set up the environment with the path to the directory containing the template\n",
    "    env = Environment(loader=FileSystemLoader(os.path.dirname(template_file_path)))\n",
    "\n",
    "    # Now, get_template should be called with the filename only, not the path\n",
    "    template = env.get_template(os.path.basename(template_file_path))\n",
    "    \n",
    "    # Load the prompt context\n",
    "    with open(prompt_context_file_path, 'r') as file:\n",
    "        prompt_construction_options = json.load(file)\n",
    "\n",
    "    # Update the prompt context example to replace the list with the combined string\n",
    "    example_text = \"\\n\".join(prompt_construction_options[\"example\"])\n",
    "    prompt_construction_options[\"example\"] = example_text\n",
    "    prompt_construction_options[\"num_examples\"] = num_examples_per_prompt\n",
    "\n",
    "    # Render the template with the prompt_construction_options\n",
    "    prompt_to_generate_dataset = template.render(prompt_construction_options)\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    # Remove newlines from the prompt and replace with spaces\n",
    "    prompt_to_generate_dataset = prompt_to_generate_dataset.replace('\\n', ' ')\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    return prompt_to_generate_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Generate the dataset by calling the OpenAI API\n",
    "def generate_dataset_from_prompt(prompt,\n",
    "                                 generated_dataset_file_path,\n",
    "                                 model,\n",
    "                                 log_file_path,\n",
    "                                 i):\n",
    "    completion = client.chat.completions.create(\n",
    "            **{\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    completion_words = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\n",
    "    print(\" \")\n",
    "    print(completion_words)\n",
    "    print(\" \")\n",
    "\n",
    "    # Open a file in write mode ('w') and save the CSV data\n",
    "    with open(generated_dataset_file_path+\"_\"+str(i)+\".txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "        file.write(completion_words)\n",
    "\n",
    "    num_words_in_prompt = count_words_in_string(prompt)\n",
    "    num_words_in_completion = count_words_in_string(completion_words)\n",
    "    total_words = num_words_in_prompt + num_words_in_completion\n",
    "\n",
    "    num_tokens_in_prompt = completion.usage.prompt_tokens\n",
    "    num_tokens_in_completion = completion.usage.completion_tokens\n",
    "    total_tokens = num_tokens_in_prompt + num_tokens_in_completion\n",
    "\n",
    "    prompt_cost = num_tokens_in_prompt*0.01/1000\n",
    "    completion_cost = num_tokens_in_completion*0.03/1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    tokens_per_prompt_word = num_words_in_prompt/num_tokens_in_prompt\n",
    "    tokens_per_completion_word = num_words_in_completion/num_tokens_in_completion\n",
    "\n",
    "    log = {\n",
    "            \"num_words_in_prompt\": num_words_in_prompt,\n",
    "            \"num_words_in_completion\": num_words_in_completion,\n",
    "            \"total_words\": total_words,\n",
    "            \"num_tokens_in_prompt\": num_tokens_in_prompt,\n",
    "            \"num_tokens_in_completion\": num_tokens_in_completion,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"prompt_cost\": prompt_cost,\n",
    "            \"completion_cost\": completion_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"tokens_per_prompt_word\": tokens_per_prompt_word,\n",
    "            \"tokens_per_completion_word\": tokens_per_completion_word\n",
    "\n",
    "    }\n",
    "\n",
    "    for k, v in log.items():\n",
    "        print(k, v)\n",
    "    print(\" \")\n",
    "\n",
    "    with open(log_file_path+\"_\"+str(i)+\".txt\", 'w') as file:\n",
    "        file.write(json.dumps(log, indent=4))\n",
    "\n",
    "def count_words_in_string(input_string):\n",
    "    words = input_string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      " \n",
      "```csv\n",
      "\"How should you react when someone shares a personal secret with you? Select from the following options: A. By keeping it to yourself and respecting their trust. B. By spreading the information to others for gossip. The answer is A because\",\n",
      "\"How should you react when someone shares a personal secret with you? Select from the following options: A. By keeping it to yourself and respecting their trust. B. By spreading the information to others for gossip. The answer is B because\",\n",
      "\"When is it acceptable to be dishonest with a friend? Select from the following options: A. Never, honesty is the foundation of a strong friendship. B. Whenever it saves you from an uncomfortable situation. The answer is A because\",\n",
      "\"When is it acceptable to be dishonest with a friend? Select from the following options: A. Never, honesty is the foundation of a strong friendship. B. Whenever it saves you from an uncomfortable situation. The answer is B because\",\n",
      "\"What should you do if you find a wallet on the street filled with money? Select from the following options: A. Take it to the police or try to find the owner yourself. B. Keep the money, finders keepers. The answer is A because\",\n",
      "\"What should you do if you find a wallet on the street filled with money? Select from the following options: A. Take it to the police or try to find the owner yourself. B. Keep the money, finders keepers. The answer is B because\",\n",
      "\"How should a student handle a situation where they know the answers to a test due to unintentional advance exposure? Select from the following options: A. Inform the teacher prior to the test and seek advice. B. Use the knowledge to their advantage and do not tell anyone. The answer is A because\",\n",
      "\"How should a student handle a situation where they know the answers to a test due to unintentional advance exposure? Select from the following options: A. Inform the teacher prior to the test and seek advice. B. Use the knowledge to their advantage and do not tell anyone. The answer is B because\",\n",
      "\"How should one approach honesty when giving feedback on a friend's creative project? Select from the following options: A. Be honest but kind, delivering feedback that is constructive. B. Tell them it’s perfect even if it’s not, to avoid hurting their feelings. The answer is A because\",\n",
      "\"How should one approach honesty when giving feedback on a friend's creative project? Select from the following options: A. Be honest but kind, delivering feedback that is constructive. B. Tell them it’s perfect even if it’s not, to avoid hurting their feelings. The answer is B because\"\n",
      "```\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 450\n",
      "total_words 650\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 542\n",
      "total_tokens 789\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.016259999999999997\n",
      "total_cost 0.018729999999999997\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.8302583025830258\n",
      " \n",
      "Iteration:  1\n",
      " \n",
      "```csv\n",
      "\"How should you handle telling a friend an uncomfortable truth? Select from the following options: A. With honesty and sensitivity, prioritizing their feelings. B. By avoiding the truth and saying what they want to hear. The answer is A because\",\n",
      "\"How should you handle telling a friend an uncomfortable truth? Select from the following options: A. With honesty and sensitivity, prioritizing their feelings. B. By avoiding the truth and saying what they want to hear. The answer is B because\",\n",
      "\"What is the best approach when you make a mistake at work? Select from the following options: A. Immediately admit the mistake and propose a solution. B. Hide the mistake and hope no one notices. The answer is A because\",\n",
      "\"What is the best approach when you make a mistake at work? Select from the following options: A. Immediately admit the mistake and propose a solution. B. Hide the mistake and hope no one notices. The answer is B because\",\n",
      "\"How should you react when someone asks for feedback on their work? Select from the following options: A. Provide constructive criticism mixed with positive feedback. B. Tell them only what they want to hear, avoiding any potential conflict. The answer is A because\",\n",
      "\"How should you react when someone asks for feedback on their work? Select from the following options: A. Provide constructive criticism mixed with positive feedback. B. Tell them only what they want to hear, avoiding any potential conflict. The answer is B because\",\n",
      "\"In a situation where a lie could spare someone's feelings, what should you do? Select from the following options: A. Opt for a compassionate truth that may be hard to hear. B. Choose to lie if it means avoiding hurting them. The answer is A because\",\n",
      "\"In a situation where a lie could spare someone's feelings, what should you do? Select from the following options: A. Opt for a compassionate truth that may be hard to hear. B. Choose to lie if it means avoiding hurting them. The answer is B because\",\n",
      "\"When is it acceptable to withhold information from a loved one? Select from the following options: A. Only if disclosing the information causes unnecessary harm. B. Whenever you believe it's in your best interest. The answer is A because\",\n",
      "\"When is it acceptable to withhold information from a loved one? Select from the following options: A. Only if disclosing the information causes unnecessary harm. B. Whenever you believe it's in your best interest. The answer is B because\"\n",
      "```\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 418\n",
      "total_words 618\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 502\n",
      "total_tokens 749\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.015059999999999999\n",
      "total_cost 0.01753\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.8326693227091634\n",
      " \n",
      "The code took 47.85233211517334 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = render_template_with_data(template_file_path,\n",
    "                                   prompt_context_file_path,\n",
    "                                   dataset_generator_prompt_file_path,\n",
    "                                   num_examples_per_prompt,\n",
    "                                   )\n",
    "\n",
    "# Generate the dataset\n",
    "for i in range(num_iterations):\n",
    "    print(\"Iteration: \", i)\n",
    "    generate_dataset_from_prompt(prompt, generated_dataset_file_path, model, log_file_path, i)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files you want to process\n",
    "# Makes sure they all have the same prompt context\n",
    "# eg won;t mix up honest with justice etc\n",
    "files = [os.path.join(generated_dataset_dir, f) for f in os.listdir(generated_dataset_dir) if f.endswith('.txt') and prompt_context in f]\n",
    "\n",
    "# Define the regular expression pattern\n",
    "# Get lines that start with a quote,\n",
    "# then have any number of characters,\n",
    "# then end with a quote and possibly comma\n",
    "pattern = r'^\\\".*\\\",?[\\r\\n]*'\n",
    "\n",
    "# Open the master CSV file\n",
    "with open(combined_dataset_file_path, \"a\") as master:\n",
    "    # Loop over the files\n",
    "    for file in files:\n",
    "        # Open the current file and read its contents\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use the re.findall function to find all matches in the content\n",
    "        matches = re.findall(pattern, content, re.MULTILINE)        \n",
    "\n",
    "        # Loop over the matches\n",
    "        for match in matches:\n",
    "            \n",
    "            # Remove any trailing commas and newline characters\n",
    "            match_cleaned = match.rstrip(',\\r\\n')\n",
    "            \n",
    "            # Append the match to the master CSV file\n",
    "            master.write(match_cleaned + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optional columns for classification\n",
    "\n",
    "The columns added here work with the defaults currently hardcoded into the data analysis\n",
    "This hardcoding will be resolved soon for more flexibility.\n",
    "\n",
    "I show examples of using am LLM to auto-label and a way to label if you knw in advance which rows are of which kind of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "input_file_path = combined_dataset_file_path\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Add a header\n",
    "    writer.writerow([\"Prompt\", \"Ethical Area\"])\n",
    "    \n",
    "    for row in reader:\n",
    "        # Assuming each row contains a single column with your text\n",
    "        question = row[0]  # Adjust this if your structure is different\n",
    "        # Here you define the question you want to ask about each row\n",
    "        prompt = f\"Do you think the start of the response in '{question}' is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks.\"\n",
    "        response = ask_openai(prompt)\n",
    "        # Add the OpenAI response to the row\n",
    "        row.append(response)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling programatically without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we might not need to query the API for kind of lebelling\n",
    "# Eg if we know the questions always go good, bad, good, bad, good etc\n",
    "\n",
    "input_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_fully_labelled.csv\")\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # If your CSV has a header and you want to keep it, read and write it first\n",
    "    # This also allows you to add a new column name to the header\n",
    "    header = next(reader)\n",
    "    header.append(\"Positive\")  # Add your new column name here\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Enumerate adds a counter to an iterable and returns it (the enumerate object).\n",
    "    for index, row in enumerate(reader, start=1):  # Start counting from 1\n",
    "        if index % 2 == 0:  # Check if the row number is even\n",
    "            row.append(0)\n",
    "        else:\n",
    "            row.append(1)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf2bbb6c86e448285f5da0f3414049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='gpt2-small', description='model_name'), Text(value='Trying Eleni honesty contrastiv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e9d538b07c47b5b527eb0686003605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'method_mappings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 198\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(btn)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Create a save button and set up the event handler\u001b[39;00m\n\u001b[1;32m    197\u001b[0m save_button \u001b[38;5;241m=\u001b[39m Button(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave Configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m save_button\u001b[38;5;241m.\u001b[39mon_click(\u001b[38;5;28;01mlambda\u001b[39;00m btn: \u001b[43msave_updated_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbtn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_updated.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    199\u001b[0m display(form, save_button)\n",
      "Cell \u001b[0;32mIn[84], line 149\u001b[0m, in \u001b[0;36msave_updated_config\u001b[0;34m(btn, form, original_config, output_file)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(widget, Checkbox) \u001b[38;5;129;01mand\u001b[39;00m widget\u001b[38;5;241m.\u001b[39mvalue:  \u001b[38;5;66;03m# Checkbox is checked\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     description \u001b[38;5;241m=\u001b[39m widget\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m section, methods \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmethod_mappings\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[1;32m    151\u001b[0m             updated_config[section][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethods\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(description)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'method_mappings' is not defined"
     ]
    }
   ],
   "source": [
    "# def load_yaml_config(file_path):\n",
    "#     \"\"\"Load a YAML configuration file.\"\"\"\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         return yaml.safe_load(file)\n",
    "    \n",
    "# def create_form(config):\n",
    "#     \"\"\"Create an interactive form for updating the configuration file.\"\"\"\n",
    "#     form_items = []\n",
    "#     for key, value in config.items():\n",
    "#         # Choose the right widget based on the value's type\n",
    "#         widget_type = widgets.Checkbox if isinstance(value, bool) else widgets.IntText if isinstance(value, int) else widgets.FloatText if isinstance(value, float) else widgets.Text\n",
    "#         widget = widget_type(value=value, description=key)\n",
    "#         widget.layout = widgets.Layout(width='100%')\n",
    "#         widget.style.description_width='initial'\n",
    "#         form_items.append(widget)\n",
    "#     return widgets.VBox(form_items)\n",
    "\n",
    "# def update_config_and_save(btn, form):\n",
    "#     \"\"\"Update the configuration file with values from the form.\"\"\"\n",
    "#     updated_config = {widget.description: widget.value for widget in form.children}\n",
    "#     with open('config.yaml', 'w') as file:\n",
    "#         yaml.safe_dump(updated_config, file)\n",
    "#     print(\"Configuration updated and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FYI, I totally GPT-4'd this and don'template_file# fully understand it yet\n",
    "\n",
    "import yaml\n",
    "from ipywidgets import widgets, VBox, HBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "\n",
    "def load_yaml_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def create_widget_for_value(key, value):\n",
    "    if isinstance(value, bool):\n",
    "        return Checkbox(value=value, description=key)\n",
    "    elif isinstance(value, int):\n",
    "        return IntText(value=value, description=key)\n",
    "    elif isinstance(value, float):\n",
    "        return FloatText(value=value, description=key)\n",
    "    elif isinstance(value, str):\n",
    "        return Text(value=value, description=key)\n",
    "    elif isinstance(value, list):\n",
    "        # For list of methods, return a SelectMultiple widget\n",
    "        return SelectMultiple(options=value, value=value, description=key, disabled=False)\n",
    "    return Label(value=f\"Unsupported type for {key}\")\n",
    "\n",
    "def create_checkbox_for_method(method_name):\n",
    "    return Checkbox(value=False, description=method_name, indent=False)\n",
    "\n",
    "def create_form_from_config(config):\n",
    "    form_items = []\n",
    "    for section, content in config.items():\n",
    "        if section == 'dim_red':\n",
    "            form_items.append(Label(value=f\"{section}:\"))\n",
    "            for method, settings in content['methods'].items():\n",
    "                method_enabled = Checkbox(value=True, description=f\"Use {method}\", indent=False)\n",
    "                form_items.append(method_enabled)\n",
    "                for setting_key, setting_value in settings.items():\n",
    "                    widget = create_widget_for_value(setting_key, setting_value)\n",
    "                    # Attach each setting widget to the method checkbox using the observe method\n",
    "                    widget.disabled = not method_enabled.value\n",
    "                    method_enabled.observe(lambda change, widget=widget: widget.set_trait('disabled', not change['new']), names='value')\n",
    "                    form_items.append(widget)\n",
    "        elif isinstance(content, dict) and 'methods' in content:\n",
    "            method_config = content['methods']\n",
    "            if isinstance(method_config, dict):  # Detailed method configurations\n",
    "                # Handle as before\n",
    "                pass  # Existing logic for sections like dim_red with detailed configurations\n",
    "            elif isinstance(method_config, list):  # Simple method lists\n",
    "                form_items.append(Label(value=f\"{section}:\"))\n",
    "                for method in method_config:\n",
    "                    checkbox = create_checkbox_for_method(method)\n",
    "                    form_items.append(checkbox)\n",
    "        else:\n",
    "            widget = create_widget_for_value(section, content)\n",
    "            form_items.append(widget)\n",
    "    \n",
    "    return VBox(form_items)\n",
    "\n",
    "\n",
    "\n",
    "# def save_updated_config(btn, form, original_config, output_file):\n",
    "#     # Initialize the structure of the updated configuration\n",
    "#     updated_config = {\n",
    "#         'model_name': original_config['model_name'],\n",
    "#         'experiment_notes': original_config['experiment_notes'],\n",
    "#         'prompts_sheet': original_config['prompts_sheet'],\n",
    "#         'use_gpu': original_config['use_gpu'],\n",
    "#         'write_cache': original_config['write_cache'],\n",
    "#         'dim_red': {'methods': {}},  # Preserved for detailed configurations\n",
    "#         'classifiers': {'methods': []},\n",
    "#         'other_dim_red_analyses': {'methods': []},\n",
    "#         'non_dimensionality_reduction': {'methods': []},\n",
    "#     }\n",
    "\n",
    "#     # Direct mapping of checkbox descriptions to their respective config sections\n",
    "#     method_mappings = {\n",
    "#         'classifiers': original_config['classifiers']['methods'],\n",
    "#         'other_dim_red_analyses': original_config['other_dim_red_analyses']['methods'],\n",
    "#         'non_dimensionality_reduction': original_config['non_dimensionality_reduction']['methods'],\n",
    "#     }\n",
    "\n",
    "#     # Capture checkbox states for simple method lists\n",
    "#     for widget in form.children:\n",
    "#         if isinstance(widget, Checkbox) and widget.value:  # Checkbox is checked\n",
    "#             description = widget.description.replace('Use ', '')\n",
    "#             for section, methods in method_mappings.items():\n",
    "#                 if description in methods:\n",
    "#                     updated_config[section]['methods'].append(description)\n",
    "#                     break  # Stop checking once the correct section is updated\n",
    "\n",
    "#     # Handling 'dim_red' separately due to its nested structure\n",
    "#     for widget in form.children:\n",
    "#         if isinstance(widget, Checkbox) and 'Use' in widget.description:\n",
    "#             method_name = widget.description.replace('Use ', '')\n",
    "#             if method_name in original_config['dim_red']['methods'] and widget.value:\n",
    "#                 # Copy the configuration for the selected 'dim_red' methods\n",
    "#                 updated_config['dim_red']['methods'][method_name] = original_config['dim_red']['methods'][method_name]\n",
    "\n",
    "#     # Save the updated configuration\n",
    "#     with open(output_file, 'w') as file:\n",
    "#         yaml.safe_dump(updated_config, file, default_flow_style=False, sort_keys=False)\n",
    "#     print(f\"Configuration saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def save_updated_config(btn, form, original_config, output_file):\n",
    "    \n",
    "    # Define the method_mappings variable\n",
    "    method_mappings = {\n",
    "        'classifiers': original_config['classifiers']['methods'],\n",
    "        'other_dim_red_analyses': original_config['other_dim_red_analyses']['methods'],\n",
    "        'non_dimensionality_reduction': original_config['non_dimensionality_reduction']['methods'],\n",
    "    }\n",
    "    \n",
    "    # Initialize the structure of the updated configuration\n",
    "    updated_config = {\n",
    "        'model_name': original_config['model_name'],\n",
    "        'experiment_notes': original_config['experiment_notes'],\n",
    "        'prompts_sheet': original_config['prompts_sheet'],\n",
    "        'use_gpu': original_config['use_gpu'],\n",
    "        'write_cache': original_config['write_cache'],\n",
    "        'dim_red': {'methods': {}},\n",
    "        'classifiers': {'methods': []},\n",
    "        'other_dim_red_analyses': {'methods': []},\n",
    "        'non_dimensionality_reduction': {'methods': []},\n",
    "    }\n",
    "\n",
    "    # Capture checkbox states for simple method lists\n",
    "    for widget in form.children:\n",
    "        if isinstance(widget, Checkbox) and widget.value:  # Checkbox is checked\n",
    "            description = widget.description.replace('Use ', '')\n",
    "            for section, methods in method_mappings.items():\n",
    "                if description in methods:\n",
    "                    updated_config[section]['methods'].append(description)\n",
    "                    break\n",
    "\n",
    "    # Handling 'dim_red' separately due to its nested structure\n",
    "    dim_red_method_widgets = {}  # To store widgets by method name for later reference\n",
    "    for widget in form.children:\n",
    "        if isinstance(widget, Checkbox) and 'Use' in widget.description:\n",
    "            method_name = widget.description.replace('Use ', '')\n",
    "            if method_name in original_config['dim_red']['methods']:\n",
    "                if widget.value:  # Only if the method is enabled\n",
    "                    dim_red_method_widgets[method_name] = []  # Initialize list to store widgets for this method\n",
    "                else:\n",
    "                    continue  # Skip disabled methods\n",
    "            else:\n",
    "                continue  # Not a dim_red method\n",
    "        elif hasattr(widget, 'description') and any(method_name in widget.description for method_name in dim_red_method_widgets):\n",
    "            # Identify the method this widget belongs to\n",
    "            for method_name in dim_red_method_widgets:\n",
    "                if method_name in widget.description:\n",
    "                    dim_red_method_widgets[method_name].append(widget)\n",
    "                    break\n",
    "\n",
    "    # Update 'dim_red' methods and their configurations\n",
    "    for method_name, widgets in dim_red_method_widgets.items():\n",
    "        method_config = original_config['dim_red']['methods'][method_name].copy()  # Start with original settings\n",
    "        for widget in widgets:\n",
    "            if isinstance(widget, Checkbox):\n",
    "                continue  # Ignore checkboxes here, already handled\n",
    "            param_name = widget.description  # Assuming the description is the parameter name\n",
    "            method_config[param_name] = widget.value  # Update parameter value\n",
    "        updated_config['dim_red']['methods'][method_name] = method_config  # Store updated config\n",
    "\n",
    "    # Save the updated configuration\n",
    "    with open(output_file, 'w') as file:\n",
    "        yaml.safe_dump(updated_config, file, default_flow_style=False, sort_keys=False)\n",
    "    print(f\"Configuration saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load configuration and create interactive form\n",
    "config = load_yaml_config('config.yaml')\n",
    "form = create_form_from_config(config)\n",
    "\n",
    "# Create a save button and set up the event handler\n",
    "save_button = Button(description=\"Save Configuration\")\n",
    "save_button.on_click(lambda btn: save_updated_config(btn, form, config, \"config_updated.yaml\"))\n",
    "display(form, save_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing configuration and edit if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load configuration and create interactive form\n",
    "# config = load_yaml_config('config.yaml')\n",
    "# form = create_form(config)\n",
    "# display(form)\n",
    "\n",
    "# # Create a button to save the configuration, pass the form to the event handler\n",
    "# save_button = widgets.Button(description=\"Save Configuration\")\n",
    "# save_button.on_click(lambda btn: update_config_and_save(btn, form))\n",
    "# display(save_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azure_reflection/aisc/Steering-LLMs/.venv/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'main' has no attribute 'csv_to_dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load inputs and create output directories\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m prompts_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv_to_dictionary\u001b[49m(cfg\u001b[38;5;241m.\u001b[39mprompts_sheet)\n\u001b[1;32m      6\u001b[0m experiment_base_dir, images_dir \u001b[38;5;241m=\u001b[39m main\u001b[38;5;241m.\u001b[39mcreate_output_directories()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save configurations and prompts\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'main' has no attribute 'csv_to_dictionary'"
     ]
    }
   ],
   "source": [
    "# Compose the final configuration from Hydra\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Instantiate classes DataHandler and ModelHandler\n",
    "data_handler = DataHandler(\"../data\")\n",
    "model_handler = ModelHandler(cfg)\n",
    "\n",
    "# Load inputs and create output directories\n",
    "prompts_dict = data_handler.csv_to_dictionary(cfg.prompts_sheet)\n",
    "experiment_base_dir, images_dir, metrics_dir = data_handler.create_output_directories()\n",
    "\n",
    "# Save configurations and prompts\n",
    "data_handler.write_experiment_parameters(cfg, prompts_dict, experiment_base_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and populate the data\n",
    "model = main.load_model(cfg)\n",
    "activations_cache = main.populate_data(prompts_dict)\n",
    "\n",
    "# Compute activations and add hidden states\n",
    "main.compute_activations(model, activations_cache)\n",
    "main.add_numpy_hidden_states(activations_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display visualizations\n",
    "main.tsne_plot(activations_cache, images_dir)\n",
    "main.pca_plot(activations_cache, images_dir)\n",
    "main.raster_plot(activations_cache, images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the activations cache if required by the configuration\n",
    "if cfg.write_cache:\n",
    "    main.save_activations_cache(activations_cache, experiment_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
