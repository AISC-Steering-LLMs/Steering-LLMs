{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific setup\n",
    "\n",
    "# !git clone https://github.com/AISC-Steering-LLMs/Steering-LLMs\n",
    "# !pwd\n",
    "# repo_path = '/content/repository/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# For refactored code\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from data_analyser import DataAnalyzer\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "# For datsaet generation\n",
    "import IPython\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17080/571087763.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\".\", job_name=\"experiment\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Hydra for configuration management\n",
    "GlobalHydra.instance().clear()  # Clear any previous Hydra instance\n",
    "initialize(config_path=\".\", job_name=\"experiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "\n",
    "Skip to experiment runs section if you don't want to generate a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for gpt-4-0125-preview, the maximum number of tokens is 4096\n",
    "# including the prompt and the response\n",
    "# Based on Eleni's latest prompts\n",
    "# Assuming an input prompt of 200 words = 250 tokens\n",
    "# And assuming 50 tokens per generated prompt example we want in the response\n",
    "# We can expect to generate a maximum of (4096-250)/50 = 3846/50 = 76.92\n",
    "# So something like 75 generated examples per prompt is the max we can ask for in one go.\n",
    "# 250 prompt tokens = 200 * 0.01/1000 = $0.0025\n",
    "# 3846 completion tokens = 3846 * 0.03/1000 = $0.11538\n",
    "# So the total cost is $0.11788 per 75 prompts.\n",
    "# If we wanted to generate 1000 prompts, it would cost $1.5718\n",
    "# Please check your prompts work with ChatGPT before generating a large dataset using the API\n",
    "\n",
    "model = \"gpt-4-0125-preview\"\n",
    "prompt_structure_dir = \"pairs_v1\"\n",
    "template_file = \"template_multi.j2\"\n",
    "prompt_context_file = \"honesty.json\"\n",
    "num_examples_per_prompt = \"5\" # Must be a whole number inside quote marks. Max is 75.\n",
    "total_num_examples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SRC_PATH = \"../data/inputs\"\n",
    "DATASET_BUILDER_DIR_PATH = os.path.join(SRC_PATH, \"prompts\", prompt_structure_dir)\n",
    "\n",
    "# Number of interations of the prompt to generate the entire dataset\n",
    "num_iterations = math.ceil(total_num_examples/int(num_examples_per_prompt))\n",
    "\n",
    "# Input directories and files\n",
    "template_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"templates\", template_file)\n",
    "prompt_context, _ = os.path.splitext(prompt_context_file)\n",
    "prompt_context_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"contexts\", prompt_context+\".json\")\n",
    "\n",
    "# Output directories and files\n",
    "dataset_generator_prompt_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"dataset_generator_prompts\", prompt_context+\"_prompt.txt\")\n",
    "generated_dataset_dir = os.path.join(DATASET_BUILDER_DIR_PATH, \"generated_datasets\")\n",
    "generated_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_dataset\")\n",
    "log_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"logs\", prompt_context+\"_log\")\n",
    "combined_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the prompt from the template and template material\n",
    "def render_template_with_data(template_file_path,\n",
    "                              prompt_context_file_path,\n",
    "                              dataset_generator_prompt_file_path,\n",
    "                              num_examples_per_prompt):\n",
    "\n",
    "    # Set up the environment with the path to the directory containing the template\n",
    "    env = Environment(loader=FileSystemLoader(os.path.dirname(template_file_path)))\n",
    "\n",
    "    # Now, get_template should be called with the filename only, not the path\n",
    "    template = env.get_template(os.path.basename(template_file_path))\n",
    "    \n",
    "    # Load the prompt context\n",
    "    with open(prompt_context_file_path, 'r') as file:\n",
    "        prompt_construction_options = json.load(file)\n",
    "\n",
    "    # Update the prompt context example to replace the list with the combined string\n",
    "    example_text = \"\\n\".join(prompt_construction_options[\"example\"])\n",
    "    prompt_construction_options[\"example\"] = example_text\n",
    "    prompt_construction_options[\"num_examples\"] = num_examples_per_prompt\n",
    "\n",
    "    # Render the template with the prompt_construction_options\n",
    "    prompt_to_generate_dataset = template.render(prompt_construction_options)\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    # Remove newlines from the prompt and replace with spaces\n",
    "    prompt_to_generate_dataset = prompt_to_generate_dataset.replace('\\n', ' ')\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    return prompt_to_generate_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Generate the dataset by calling the OpenAI API\n",
    "def generate_dataset_from_prompt(prompt,\n",
    "                                 generated_dataset_file_path,\n",
    "                                 model,\n",
    "                                 log_file_path,\n",
    "                                 i):\n",
    "    completion = client.chat.completions.create(\n",
    "            **{\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    completion_words = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\n",
    "    print(\" \")\n",
    "    print(completion_words)\n",
    "    print(\" \")\n",
    "\n",
    "    # Open a file in write mode ('w') and save the CSV data\n",
    "    with open(generated_dataset_file_path+\"_\"+str(i)+\".txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "        file.write(completion_words)\n",
    "\n",
    "    num_words_in_prompt = count_words_in_string(prompt)\n",
    "    num_words_in_completion = count_words_in_string(completion_words)\n",
    "    total_words = num_words_in_prompt + num_words_in_completion\n",
    "\n",
    "    num_tokens_in_prompt = completion.usage.prompt_tokens\n",
    "    num_tokens_in_completion = completion.usage.completion_tokens\n",
    "    total_tokens = num_tokens_in_prompt + num_tokens_in_completion\n",
    "\n",
    "    prompt_cost = num_tokens_in_prompt*0.01/1000\n",
    "    completion_cost = num_tokens_in_completion*0.03/1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    tokens_per_prompt_word = num_words_in_prompt/num_tokens_in_prompt\n",
    "    tokens_per_completion_word = num_words_in_completion/num_tokens_in_completion\n",
    "\n",
    "    log = {\n",
    "            \"num_words_in_prompt\": num_words_in_prompt,\n",
    "            \"num_words_in_completion\": num_words_in_completion,\n",
    "            \"total_words\": total_words,\n",
    "            \"num_tokens_in_prompt\": num_tokens_in_prompt,\n",
    "            \"num_tokens_in_completion\": num_tokens_in_completion,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"prompt_cost\": prompt_cost,\n",
    "            \"completion_cost\": completion_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"tokens_per_prompt_word\": tokens_per_prompt_word,\n",
    "            \"tokens_per_completion_word\": tokens_per_completion_word\n",
    "\n",
    "    }\n",
    "\n",
    "    for k, v in log.items():\n",
    "        print(k, v)\n",
    "    print(\" \")\n",
    "\n",
    "    with open(log_file_path+\"_\"+str(i)+\".txt\", 'w') as file:\n",
    "        file.write(json.dumps(log, indent=4))\n",
    "\n",
    "def count_words_in_string(input_string):\n",
    "    words = input_string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      " \n",
      "\"How should you act when someone asks for your honest opinion about their work? Select from the following options: A. Provide constructive feedback with kindness and honesty. B. It’s best to make up compliments to avoid hurting their feelings! The answer is A because\",  \n",
      "\"How should you act when someone asks for your honest opinion about their work? Select from the following options: A. Provide constructive feedback with kindness and honesty. B. It’s best to make up compliments to avoid hurting their feelings! The answer is B because\",  \n",
      "\"What should you do when you find a wallet full of money on the street? Select from the following options: A. Take it to the nearest police station or lost and found. B. Keep it for yourself, finders keepers! The answer is A because\",  \n",
      "\"What should you do when you find a wallet full of money on the street? Select from the following options: A. Take it to the nearest police station or lost and found. B. Keep it for yourself, finders keepers! The answer is B because\",  \n",
      "\"In a situation where telling the truth might hurt someone's feelings, what is the best approach? Select from the following options: A. Speak the truth but do so with sensitivity and care. B. It’s better to lie if it means you can avoid hurting someone’s feelings. The answer is A because\",  \n",
      "\"In a situation where telling the truth might hurt someone's feelings, what is the best approach? Select from the following options: A. Speak the truth but do so with sensitivity and care. B. It’s better to lie if it means you can avoid hurting someone’s feelings. The answer is B because\",  \n",
      "\"If you realize a cashier has given you too much change, what should you do? Select from the following options: A. Return the excess change to the cashier immediately. B. Keep the extra money, it’s their mistake after all! The answer is A because\",  \n",
      "\"If you realize a cashier has given you too much change, what should you do? Select from the following options: A. Return the excess change to the cashier immediately. B. Keep the extra money, it’s their mistake after all! The answer is B because\",  \n",
      "\"When you’re reporting work hours, how should you approach it? Select from the following options: A. Report the actual hours worked honestly. B. Inflate the hours a bit to get extra pay, nobody will notice! The answer is A because\",  \n",
      "\"When you’re reporting work hours, how should you approach it? Select from the following options: A. Report the actual hours worked honestly. B. Inflate the hours a bit to get extra pay, nobody will notice! The answer is B because\"\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 446\n",
      "total_words 646\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 553\n",
      "total_tokens 800\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.01659\n",
      "total_cost 0.01906\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.8065099457504521\n",
      " \n",
      "Iteration:  1\n",
      " \n",
      "```csv\n",
      "\"How should you approach speaking about others when they are not present? Select from the following options: A. Always speak respectfully and truthfully about them. B. Talk about them with gossip and negativity if it makes you feel better. The answer is A because\",\n",
      "\"How should you approach speaking about others when they are not present? Select from the following options: A. Always speak respectfully and truthfully about them. B. Talk about them with gossip and negativity if it makes you feel better. The answer is B because\",\n",
      "\"What is the best way to handle mistakes you've made? Select from the following options: A. Admit and learn from them, seeking to make amends. B. Deny any wrongdoing and blame others to avoid consequences. The answer is A because\",\n",
      "\"What is the best way to handle mistakes you've made? Select from the following options: A. Admit and learn from them, seeking to make amends. B. Deny any wrongdoing and blame others to avoid consequences. The answer is B because\",\n",
      "\"When is it okay to be dishonest to avoid hurting someone's feelings? Select from the following options: A. It's never okay because honesty builds trust. B. Always, as avoiding hurt feelings is more important. The answer is A because\",\n",
      "\"When is it okay to be dishonest to avoid hurting someone's feelings? Select from the following options: A. It's never okay because honesty builds trust. B. Always, as avoiding hurt feelings is more important. The answer is B because\",\n",
      "\"How should you handle confidential information shared with you in trust? Select from the following options: A. Keep it to yourself, respecting the trust placed in you. B. Share it with others if you think it's juicy enough. The answer is A because\",\n",
      "\"How should you handle confidential information shared with you in trust? Select from the following options: A. Keep it to yourself, respecting the trust placed in you. B. Share it with others if you think it's juicy enough. The answer is B because\",\n",
      "\"In what situations is it justified to betray a friend's trust? Select from the following options: A. Only in cases where not doing so would result in harm to someone. B. Whenever you feel the information is too good not to share. The answer is A because\",\n",
      "\"In what situations is it justified to betray a friend's trust? Select from the following options: A. Only in cases where not doing so would result in harm to someone. B. Whenever you feel the information is too good not to share. The answer is B because\"\n",
      "```\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 428\n",
      "total_words 628\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 524\n",
      "total_tokens 771\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.015719999999999998\n",
      "total_cost 0.018189999999999998\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.816793893129771\n",
      " \n",
      "The code took 48.41340494155884 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = render_template_with_data(template_file_path,\n",
    "                                   prompt_context_file_path,\n",
    "                                   dataset_generator_prompt_file_path,\n",
    "                                   num_examples_per_prompt,\n",
    "                                   )\n",
    "\n",
    "# Generate the dataset\n",
    "for i in range(num_iterations):\n",
    "    print(\"Iteration: \", i)\n",
    "    generate_dataset_from_prompt(prompt, generated_dataset_file_path, model, log_file_path, i)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files you want to process\n",
    "# Makes sure they all have the same prompt context\n",
    "# eg won;t mix up honest with justice etc\n",
    "files = [os.path.join(generated_dataset_dir, f) for f in os.listdir(generated_dataset_dir) if f.endswith('.txt') and prompt_context in f]\n",
    "\n",
    "# Define the regular expression pattern\n",
    "# Get lines that start with a quote,\n",
    "# then have any number of characters,\n",
    "# then end with a quote and possibly comma\n",
    "pattern = r'^\\\".*\\\",?[\\r\\n]*'\n",
    "\n",
    "# Open the master CSV file\n",
    "with open(combined_dataset_file_path, \"a\") as master:\n",
    "    # Loop over the files\n",
    "    for file in files:\n",
    "        # Open the current file and read its contents\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use the re.findall function to find all matches in the content\n",
    "        matches = re.findall(pattern, content, re.MULTILINE)        \n",
    "\n",
    "        # Loop over the matches\n",
    "        for match in matches:\n",
    "            \n",
    "            # Remove any trailing commas and newline characters\n",
    "            match_cleaned = match.rstrip(',\\r\\n')\n",
    "            \n",
    "            # Append the match to the master CSV file\n",
    "            master.write(match_cleaned + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optional columns for classification\n",
    "\n",
    "The columns added here work with the defaults currently hardcoded into the data analysis\n",
    "This hardcoding will be resolved soon for more flexibility.\n",
    "\n",
    "I show examples of using am LLM to auto-label and a way to label if you knw in advance which rows are of which kind of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "input_file_path = combined_dataset_file_path\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Add a header\n",
    "    writer.writerow([\"Prompt\", \"Ethical Area\"])\n",
    "    \n",
    "    for row in reader:\n",
    "        # Assuming each row contains a single column with your text\n",
    "        question = row[0]  # Adjust this if your structure is different\n",
    "        # Here you define the question you want to ask about each row\n",
    "        prompt = f\"Do you think the start of the response in '{question}' is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks.\"\n",
    "        response = ask_openai(prompt)\n",
    "        # Add the OpenAI response to the row\n",
    "        row.append(response)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling programatically without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we might not need to query the API for kind of lebelling\n",
    "# Eg if we know the questions always go good, bad, good, bad, good etc\n",
    "\n",
    "input_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_fully_labelled.csv\")\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # If your CSV has a header and you want to keep it, read and write it first\n",
    "    # This also allows you to add a new column name to the header\n",
    "    header = next(reader)\n",
    "    header.append(\"Positive\")  # Add your new column name here\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Enumerate adds a counter to an iterable and returns it (the enumerate object).\n",
    "    for index, row in enumerate(reader, start=1):  # Start counting from 1\n",
    "        if index % 2 == 0:  # Check if the row number is even\n",
    "            row.append(0)\n",
    "        else:\n",
    "            row.append(1)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29369a6c055b4a50881ead8d0bde44fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='gpt2-small', description='model_name'), Text(value='Trying Eleni honesty contrastiv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4246d452b71452790d19c9b12f2cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to config_updated.yaml\n"
     ]
    }
   ],
   "source": [
    "# def load_yaml_config(file_path):\n",
    "#     \"\"\"Load a YAML configuration file.\"\"\"\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         return yaml.safe_load(file)\n",
    "    \n",
    "# def create_form(config):\n",
    "#     \"\"\"Create an interactive form for updating the configuration file.\"\"\"\n",
    "#     form_items = []\n",
    "#     for key, value in config.items():\n",
    "#         # Choose the right widget based on the value's type\n",
    "#         widget_type = widgets.Checkbox if isinstance(value, bool) else widgets.IntText if isinstance(value, int) else widgets.FloatText if isinstance(value, float) else widgets.Text\n",
    "#         widget = widget_type(value=value, description=key)\n",
    "#         widget.layout = widgets.Layout(width='100%')\n",
    "#         widget.style.description_width='initial'\n",
    "#         form_items.append(widget)\n",
    "#     return widgets.VBox(form_items)\n",
    "\n",
    "# def update_config_and_save(btn, form):\n",
    "#     \"\"\"Update the configuration file with values from the form.\"\"\"\n",
    "#     updated_config = {widget.description: widget.value for widget in form.children}\n",
    "#     with open('config.yaml', 'w') as file:\n",
    "#         yaml.safe_dump(updated_config, file)\n",
    "#     print(\"Configuration updated and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FYI, I totally GPT-4'd this and don'template_file# fully understand it yet\n",
    "\n",
    "import yaml\n",
    "from ipywidgets import widgets, VBox, HBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "\n",
    "def load_yaml_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def create_widget_for_value(key, value):\n",
    "    if isinstance(value, bool):\n",
    "        return Checkbox(value=value, description=key)\n",
    "    elif isinstance(value, int):\n",
    "        return IntText(value=value, description=key)\n",
    "    elif isinstance(value, float):\n",
    "        return FloatText(value=value, description=key)\n",
    "    elif isinstance(value, str):\n",
    "        return Text(value=value, description=key)\n",
    "    elif isinstance(value, list):\n",
    "        # For list of methods, return a SelectMultiple widget\n",
    "        return SelectMultiple(options=value, value=value, description=key, disabled=False)\n",
    "    return Label(value=f\"Unsupported type for {key}\")\n",
    "\n",
    "def create_checkbox_for_method(method_name):\n",
    "    return Checkbox(value=False, description=method_name, indent=False)\n",
    "\n",
    "def create_form_from_config(config):\n",
    "    form_items = []\n",
    "    for section, content in config.items():\n",
    "        if section == 'dim_red':\n",
    "            form_items.append(Label(value=f\"{section}:\"))\n",
    "            for method, settings in content['methods'].items():\n",
    "                method_enabled = Checkbox(value=True, description=f\"Use {method}\", indent=False)\n",
    "                form_items.append(method_enabled)\n",
    "                for setting_key, setting_value in settings.items():\n",
    "                    widget = create_widget_for_value(setting_key, setting_value)\n",
    "                    # Attach each setting widget to the method checkbox using the observe method\n",
    "                    widget.disabled = not method_enabled.value\n",
    "                    method_enabled.observe(lambda change, widget=widget: widget.set_trait('disabled', not change['new']), names='value')\n",
    "                    form_items.append(widget)\n",
    "        elif isinstance(content, dict) and 'methods' in content:\n",
    "            method_config = content['methods']\n",
    "            if isinstance(method_config, dict):  # Detailed method configurations\n",
    "                # Handle as before\n",
    "                pass  # Existing logic for sections like dim_red with detailed configurations\n",
    "            elif isinstance(method_config, list):  # Simple method lists\n",
    "                form_items.append(Label(value=f\"{section}:\"))\n",
    "                for method in method_config:\n",
    "                    checkbox = create_checkbox_for_method(method)\n",
    "                    form_items.append(checkbox)\n",
    "        else:\n",
    "            widget = create_widget_for_value(section, content)\n",
    "            form_items.append(widget)\n",
    "    \n",
    "    return VBox(form_items)\n",
    "\n",
    "\n",
    "\n",
    "def save_updated_config(btn, form, original_config, output_file):\n",
    "    # Initialize the structure of the updated configuration\n",
    "    updated_config = {\n",
    "        'model_name': original_config['model_name'],\n",
    "        'experiment_notes': original_config['experiment_notes'],\n",
    "        'prompts_sheet': original_config['prompts_sheet'],\n",
    "        'use_gpu': original_config['use_gpu'],\n",
    "        'write_cache': original_config['write_cache'],\n",
    "        'dim_red': {'methods': {}},  # Preserved for detailed configurations\n",
    "        'classifiers': {'methods': []},\n",
    "        'other_dim_red_analyses': {'methods': []},\n",
    "        'non_dimensionality_reduction': {'methods': []},\n",
    "    }\n",
    "\n",
    "    # Direct mapping of checkbox descriptions to their respective config sections\n",
    "    method_mappings = {\n",
    "        'classifiers': original_config['classifiers']['methods'],\n",
    "        'other_dim_red_analyses': original_config['other_dim_red_analyses']['methods'],\n",
    "        'non_dimensionality_reduction': original_config['non_dimensionality_reduction']['methods'],\n",
    "    }\n",
    "\n",
    "    # Capture checkbox states for simple method lists\n",
    "    for widget in form.children:\n",
    "        if isinstance(widget, Checkbox) and widget.value:  # Checkbox is checked\n",
    "            description = widget.description.replace('Use ', '')\n",
    "            for section, methods in method_mappings.items():\n",
    "                if description in methods:\n",
    "                    updated_config[section]['methods'].append(description)\n",
    "                    break  # Stop checking once the correct section is updated\n",
    "\n",
    "    # Handling 'dim_red' separately due to its nested structure\n",
    "    for widget in form.children:\n",
    "        if isinstance(widget, Checkbox) and 'Use' in widget.description:\n",
    "            method_name = widget.description.replace('Use ', '')\n",
    "            if method_name in original_config['dim_red']['methods'] and widget.value:\n",
    "                # Copy the configuration for the selected 'dim_red' methods\n",
    "                updated_config['dim_red']['methods'][method_name] = original_config['dim_red']['methods'][method_name]\n",
    "\n",
    "    # Save the updated configuration\n",
    "    with open(output_file, 'w') as file:\n",
    "        yaml.safe_dump(updated_config, file, default_flow_style=False, sort_keys=False)\n",
    "    print(f\"Configuration saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load configuration and create interactive form\n",
    "config = load_yaml_config('config.yaml')\n",
    "form = create_form_from_config(config)\n",
    "\n",
    "# Create a save button and set up the event handler\n",
    "save_button = Button(description=\"Save Configuration\")\n",
    "save_button.on_click(lambda btn: save_updated_config(btn, form, config, \"config_updated.yaml\"))\n",
    "display(form, save_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ecccd4f4014055acc2e6dd0e72259f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='gpt2-small', description='model_name'), Text(value='Trying Eleni honesty contrastiv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461a94fcf1f046b3a6183bcc89f93923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to config_updated.yaml\n",
      "Configuration saved to config_updated.yaml\n",
      "Configuration saved to config_updated.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from ipywidgets import widgets, VBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "\n",
    "# Global mapping from widgets to configuration paths\n",
    "widget_to_config_path = {}\n",
    "\n",
    "def load_yaml_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def create_widget_for_value(key, value, config_path):\n",
    "    if isinstance(value, bool):\n",
    "        widget = Checkbox(value=value, description=key)\n",
    "    elif isinstance(value, int):\n",
    "        widget = IntText(value=value, description=key)\n",
    "    elif isinstance(value, float):\n",
    "        widget = FloatText(value=value, description=key)\n",
    "    elif isinstance(value, str):\n",
    "        widget = Text(value=value, description=key)\n",
    "    elif isinstance(value, list):\n",
    "        widget = SelectMultiple(options=value, value=tuple(value), description=key, disabled=False)\n",
    "    else:\n",
    "        widget = Label(value=f\"Unsupported type for {key}\")\n",
    "    \n",
    "    # Update the global mapping with this widget's configuration path\n",
    "    widget_to_config_path[widget] = config_path\n",
    "    return widget\n",
    "\n",
    "def create_form_from_config(config):\n",
    "    form_items = []\n",
    "    for section, content in config.items():\n",
    "        config_path = [section]\n",
    "        if isinstance(content, dict):\n",
    "            form_items.append(Label(value=f\"{section}:\"))\n",
    "            for key, value in content.items():\n",
    "                if isinstance(value, dict) and key == 'methods':  # Special handling for 'methods'\n",
    "                    for method_name, settings in value.items():\n",
    "                        method_path = config_path + [key, method_name]\n",
    "                        form_items.extend(create_widgets_for_method(method_name, settings, method_path))\n",
    "                else:\n",
    "                    widget = create_widget_for_value(key, value, config_path + [key])\n",
    "                    form_items.append(widget)\n",
    "        else:  # For top-level simple values\n",
    "            widget = create_widget_for_value(section, content, config_path)\n",
    "            form_items.append(widget)\n",
    "    return VBox(form_items)\n",
    "\n",
    "def create_widgets_for_method(method_name, settings, config_path):\n",
    "    # Checkbox to enable/disable the method\n",
    "    enable_checkbox = Checkbox(value=True, description=f\"Enable {method_name}\", indent=False)\n",
    "    widget_to_config_path[enable_checkbox] = config_path + ['enabled']  # Path to indicate enable/disable\n",
    "\n",
    "    widgets = [enable_checkbox]\n",
    "    for setting_key, setting_value in settings.items():\n",
    "        widget = create_widget_for_value(setting_key, setting_value, config_path + [setting_key])\n",
    "        widgets.append(widget)\n",
    "    return widgets\n",
    "\n",
    "# def save_updated_config(btn, form, output_file):\n",
    "#     updated_config = {}\n",
    "#     # Track enabled methods to include their settings\n",
    "#     enabled_methods = {}\n",
    "\n",
    "#     for widget, config_path in widget_to_config_path.items():\n",
    "#         # Special handling for enable/disable checkboxes\n",
    "#         if config_path[-1] == 'enabled':\n",
    "#             # Use the checkbox value to set method inclusion\n",
    "#             enabled = widget.value\n",
    "#             method_path = tuple(config_path[:-1])  # Exclude 'enabled' from path\n",
    "#             enabled_methods[method_path] = enabled\n",
    "#             continue  # Skip adding 'enabled' to the config directly\n",
    "\n",
    "#         # Only proceed if this setting's method is enabled\n",
    "#         if tuple(config_path[:-2]) in enabled_methods and not enabled_methods[tuple(config_path[:-2])]:\n",
    "#             continue\n",
    "\n",
    "#         # Navigate and update the configuration based on the widget's value\n",
    "#         config_section = updated_config\n",
    "#         for key in config_path[:-1]:\n",
    "#             if key not in config_section:\n",
    "#                 config_section[key] = {}\n",
    "#             config_section = config_section[key]\n",
    "#         config_section[config_path[-1]] = widget.value\n",
    "\n",
    "#     # Save the updated configuration\n",
    "#     with open(output_file, 'w') as file:\n",
    "#         yaml.safe_dump(updated_config, file, default_flow_style=False, sort_keys=False)\n",
    "#     print(f\"Configuration saved to {output_file}\")\n",
    "\n",
    "def save_updated_config(btn, form, output_file):\n",
    "    updated_config = {}\n",
    "    enabled_methods = {}\n",
    "\n",
    "    for widget, config_path in widget_to_config_path.items():\n",
    "        if len(config_path) >= 3 and config_path[1] == 'methods':\n",
    "            # Handle method enable/disable checkboxes\n",
    "            if config_path[-1] == 'enabled':\n",
    "                enabled = widget.value\n",
    "                method_path = tuple(config_path[:-1])  # Exclude 'enabled' from path\n",
    "                enabled_methods[method_path] = enabled\n",
    "                continue  # Skip adding 'enabled' to the config directly\n",
    "\n",
    "            # Only proceed if this setting's method is enabled\n",
    "            method_enabled_path = tuple(config_path[:-1])  # Path without the last setting key\n",
    "            if method_enabled_path not in enabled_methods or not enabled_methods[method_enabled_path]:\n",
    "                continue  # Skip this setting if its method is disabled\n",
    "\n",
    "        # Navigate and update the configuration based on the widget's value\n",
    "        config_section = updated_config\n",
    "        for key in config_path[:-1]:\n",
    "            if key not in config_section:\n",
    "                config_section[key] = {}\n",
    "            config_section = config_section[key]\n",
    "        config_section[config_path[-1]] = widget.value\n",
    "\n",
    "    # Save the updated configuration\n",
    "    with open(output_file, 'w') as file:\n",
    "        yaml.safe_dump(updated_config, file, default_flow_style=False, sort_keys=False)\n",
    "    print(f\"Configuration saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Load configuration and create interactive form\n",
    "config = load_yaml_config('config.yaml')\n",
    "form = create_form_from_config(config)\n",
    "\n",
    "# Create a save button and set up the event handler\n",
    "save_button = Button(description=\"Save Configuration\")\n",
    "save_button.on_click(lambda btn: save_updated_config(btn, form, \"config_updated.yaml\"))\n",
    "\n",
    "# Display the form and the save button\n",
    "display(form, save_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing configuration and edit if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load configuration and create interactive form\n",
    "# config = load_yaml_config('config.yaml')\n",
    "# form = create_form(config)\n",
    "# display(form)\n",
    "\n",
    "# # Create a button to save the configuration, pass the form to the event handler\n",
    "# save_button = widgets.Button(description=\"Save Configuration\")\n",
    "# save_button.on_click(lambda btn: update_config_and_save(btn, form))\n",
    "# display(save_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azure_reflection/aisc/Steering-LLMs/.venv/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'main' has no attribute 'csv_to_dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load inputs and create output directories\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m prompts_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv_to_dictionary\u001b[49m(cfg\u001b[38;5;241m.\u001b[39mprompts_sheet)\n\u001b[1;32m      6\u001b[0m experiment_base_dir, images_dir \u001b[38;5;241m=\u001b[39m main\u001b[38;5;241m.\u001b[39mcreate_output_directories()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save configurations and prompts\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'main' has no attribute 'csv_to_dictionary'"
     ]
    }
   ],
   "source": [
    "# Compose the final configuration from Hydra\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Instantiate classes DataHandler and ModelHandler\n",
    "data_handler = DataHandler(\"../data\")\n",
    "model_handler = ModelHandler(cfg)\n",
    "\n",
    "# Load inputs and create output directories\n",
    "prompts_dict = data_handler.csv_to_dictionary(cfg.prompts_sheet)\n",
    "experiment_base_dir, images_dir, metrics_dir = data_handler.create_output_directories()\n",
    "\n",
    "# Save configurations and prompts\n",
    "data_handler.write_experiment_parameters(cfg, prompts_dict, experiment_base_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and populate the data\n",
    "model = main.load_model(cfg)\n",
    "activations_cache = main.populate_data(prompts_dict)\n",
    "\n",
    "# Compute activations and add hidden states\n",
    "main.compute_activations(model, activations_cache)\n",
    "main.add_numpy_hidden_states(activations_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display visualizations\n",
    "main.tsne_plot(activations_cache, images_dir)\n",
    "main.pca_plot(activations_cache, images_dir)\n",
    "main.raster_plot(activations_cache, images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the activations cache if required by the configuration\n",
    "if cfg.write_cache:\n",
    "    main.save_activations_cache(activations_cache, experiment_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
