{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific setup\n",
    "\n",
    "# !git clone https://github.com/AISC-Steering-LLMs/Steering-LLMs\n",
    "# !pwd\n",
    "# repo_path = '/content/repository/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaleido in /home/aayushkucheria/Documents/Steering-LLMs/venv/lib/python3.10/site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%pip install kaleido\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets, Layout, Box, VBox, Label\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from data_analyser import DataAnalyzer\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_447959/571087763.py:3: UserWarning:\n",
      "\n",
      "\n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Hydra for configuration management\n",
    "GlobalHydra.instance().clear()  # Clear any previous Hydra instance\n",
    "initialize(config_path=\".\", job_name=\"experiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for styles\n",
    "widget_layout = Layout(width='100%')\n",
    "description_style = {'description_width': 'initial'}\n",
    "BASE_PATH = '../data/inputs/' \n",
    "\n",
    "\n",
    "def load_yaml_config(file_path):\n",
    "    \"\"\"Load a YAML configuration file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "def create_section_heading(title):\n",
    "    \"\"\"Create a section heading for the form.\"\"\"\n",
    "    return widgets.Label(value=title, layout=widgets.Layout(height='45px', align_items='center', justify_content='center'))\n",
    "    \n",
    "def create_form(config):\n",
    "    \"\"\"Create an interactive form for updating the configuration file.\"\"\"\n",
    "    form_items = []\n",
    "    # Define the options for the dropdown\n",
    "    model_options = ['gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-XL', 'llama']\n",
    "\n",
    "    # Data Section\n",
    "    form_items.append(create_section_heading('Data Settings'))\n",
    "    base_path_display = widgets.Text(\n",
    "        value=BASE_PATH,\n",
    "        description='Input files located at:',\n",
    "        disabled=True,\n",
    "        layout=widget_layout,\n",
    "        style=description_style\n",
    "    )\n",
    "    form_items.append(base_path_display)\n",
    "    form_items.append(widgets.Text(\n",
    "        value=config.get('prompts_sheet', ''), \n",
    "        description='prompts_sheet',\n",
    "        placeholder='Enter the filename of the dataset including its extension (e.g., .csv or .xlsx).',\n",
    "        layout=widget_layout,\n",
    "        style=description_style\n",
    "    ))\n",
    "    \n",
    "    # Model Section\n",
    "    form_items.append(create_section_heading('Model Configuration'))\n",
    "    form_items.append(widgets.Dropdown(\n",
    "        options=model_options,\n",
    "        value=config.get('model_name', 'gpt2-small'),\n",
    "        description='model_name',\n",
    "        tooltip='Select the language model you want to use.',\n",
    "        layout=widget_layout,\n",
    "        style=description_style\n",
    "    ))\n",
    "\n",
    "    # Execution Section\n",
    "    form_items.append(create_section_heading('Execution Settings'))\n",
    "    form_items.append(widgets.Checkbox(\n",
    "        value=config.get('use_gpu', False),\n",
    "        description='use_gpu',\n",
    "        tooltip='Check this box to use GPU for computation if available.',\n",
    "    ))\n",
    "    form_items.append(widgets.Checkbox(\n",
    "        value=config.get('write_cache', False),\n",
    "        description='write_cache',\n",
    "        tooltip='Check this box to write intermediate computations to disk.',\n",
    "    ))\n",
    "    form_items.append(widgets.Textarea(\n",
    "        value=config.get('experiment_notes', ''),\n",
    "        placeholder='Enter any notes for the experiment here.',\n",
    "        description='experiment_notes',\n",
    "        layout=widget_layout,\n",
    "        style=description_style\n",
    "    ))\n",
    "\n",
    "    return widgets.VBox(form_items)\n",
    "    \n",
    "\n",
    "\n",
    "def update_config_and_save(btn, form):\n",
    "    \"\"\"Update the configuration file with values from the form.\"\"\"\n",
    "    updated_config = {}\n",
    "    for widget in form.children:\n",
    "        # Skip over any non-input widgets like Labels or disabled Text widgets\n",
    "        if isinstance(widget, widgets.Label) or (isinstance(widget, widgets.Text) and widget.disabled):\n",
    "            continue\n",
    "        if isinstance(widget, widgets.Text) and widget.description == 'prompts_sheet':\n",
    "            # Concatenate the base path with the provided filename\n",
    "            updated_config[widget.description] = widget.value\n",
    "        elif isinstance(widget, (widgets.Text, widgets.Textarea, widgets.Dropdown, widgets.Checkbox)):\n",
    "            # Make sure to capture only the widgets that should contribute to the configuration\n",
    "            updated_config[widget.description] = widget.value\n",
    "    \n",
    "    # Use the filtered updated_config to write to the yaml file\n",
    "    with open('config.yaml', 'w') as file:\n",
    "        yaml.safe_dump(updated_config, file)\n",
    "    print(\"Configuration updated and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing configuration and edit if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a87e8d46ec44203999f1ffdfb6aafff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Data Settings', layout=Layout(align_items='center', height='45px', justify_content…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f8114a0a7f4de48072369632a1add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Configuration', layout=Layout(margin='10px 0', width='auto'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration updated and saved.\n",
      "Configuration updated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Load configuration and create interactive form\n",
    "config = load_yaml_config('config.yaml')\n",
    "form = create_form(config)\n",
    "display(form)\n",
    "\n",
    "# Create a button to save the configuration, pass the form to the event handler\n",
    "save_button = widgets.Button(\n",
    "        description=\"Save Configuration\",\n",
    "        button_style='success',  # Use 'success' styling for a green color\n",
    "        tooltip='Click to save the configuration',\n",
    "        layout=widgets.Layout(width='auto', margin='10px 0'))\n",
    "save_button.on_click(lambda btn: update_config_and_save(btn, form))\n",
    "display(save_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Compose the final configuration from Hydra\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Instantiate classes DataHandler and ModelHandler\n",
    "data_handler = DataHandler(\"../data\")\n",
    "model_handler = ModelHandler(cfg)\n",
    "\n",
    "# Load inputs and create output directories\n",
    "prompts_dict = data_handler.csv_to_dictionary(cfg.prompts_sheet)\n",
    "experiment_base_dir, images_dir, metrics_dir = data_handler.create_output_directories()\n",
    "\n",
    "# Save configurations and prompts\n",
    "data_handler.write_experiment_parameters(cfg, prompts_dict, experiment_base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing activations:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing activations: 100%|██████████| 160/160 [00:12<00:00, 12.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and populate the data\n",
    "activations_cache = data_handler.populate_data(prompts_dict)\n",
    "\n",
    "# Compute activations and add hidden states\n",
    "model_handler.compute_activations(activations_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzer = DataAnalyzer(images_dir, metrics_dir, 42)\n",
    "\n",
    "# Create checkboxes for each visualization type\n",
    "tsne_checkbox = widgets.Checkbox(value=False, description='t-SNE Plot')\n",
    "pca_checkbox = widgets.Checkbox(value=False, description='PCA Plot')\n",
    "fa_checkbox = widgets.Checkbox(value=False, description='Feature Agglomeration')\n",
    "raster_checkbox = widgets.Checkbox(value=False, description='Raster Plot')\n",
    "random_proj_checkbox = widgets.Checkbox(value=False, description='Random Projections Analysis')\n",
    "probe_hidden_states_checkbox = widgets.Checkbox(value=False, description='Probe Hidden States')\n",
    "classifier_battery_checkbox = widgets.Checkbox(value=False, description='Classifier Battery (needs t-SNE)')\n",
    "\n",
    "# Event handler for the button click\n",
    "def on_run_button_clicked(b):\n",
    "    # Clear previous output\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    if tsne_checkbox.value:\n",
    "        # Run t-SNE plot\n",
    "        tsne_model = TSNE(n_components=2, random_state=42)\n",
    "        tsne_embedded_data_dict, tsne_labels, tsne_prompts = data_analyzer.plot_embeddings(activations_cache, tsne_model)\n",
    "    \n",
    "    if pca_checkbox.value:\n",
    "        # Run PCA plot\n",
    "        pca_model = PCA(n_components=2, random_state=42)\n",
    "        pca_embedded_data_dict, pca_labels, pca_prompts = data_analyzer.plot_embeddings(activations_cache, pca_model)\n",
    "    \n",
    "    if fa_checkbox.value:\n",
    "        # Run Feature Agglomeration\n",
    "        fa_model = FeatureAgglomeration(n_clusters=2)\n",
    "        fa_embedded_data_dict, fa_labels, fa_prompts = data_analyzer.plot_embeddings(activations_cache, fa_model)\n",
    "    \n",
    "    if raster_checkbox.value:\n",
    "        # Run Raster plot\n",
    "        data_analyzer.raster_plot(activations_cache)\n",
    "    \n",
    "    if random_proj_checkbox.value:\n",
    "        # Run Random Projections Analysis\n",
    "        data_analyzer.random_projections_analysis(activations_cache)\n",
    "    \n",
    "    if probe_hidden_states_checkbox.value:\n",
    "        # Run Probe Hidden States\n",
    "        data_analyzer.probe_hidden_states(activations_cache)\n",
    "    \n",
    "    if classifier_battery_checkbox.value:\n",
    "        # Run Classifier Battery\n",
    "        data_analyzer.classifier_battery(tsne_embedded_data_dict, tsne_labels, tsne_prompts, 0.2)\n",
    "    \n",
    "    print(\"Selected visualizations have been executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b8a7cef31d49b6b42b74cf438d0fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=False, description='t-SNE Plot'), Checkbox(value=False, description='PCA Plot'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSNE: 100%|██████████| 12/12 [00:06<00:00,  1.75it/s]\n",
      "PCA: 100%|██████████| 12/12 [00:02<00:00,  4.41it/s]\n",
      "FeatureAgglomeration: 100%|██████████| 12/12 [00:02<00:00,  5.74it/s]\n",
      "Computing logistic_regression:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 46\u001b[0m, in \u001b[0;36mon_run_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     42\u001b[0m     data_analyzer\u001b[38;5;241m.\u001b[39mprobe_hidden_states(activations_cache)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classifier_battery_checkbox\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Run Classifier Battery\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mdata_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_battery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsne_embedded_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsne_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsne_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected visualizations have been executed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:239\u001b[0m, in \u001b[0;36mDataAnalyzer.classifier_battery\u001b[0;34m(self, embedded_data_dict, labels, prompts, test_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    238\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     metrics_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_classifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedded_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metrics_dict)\n\u001b[1;32m    241\u001b[0m     metrics_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:292\u001b[0m, in \u001b[0;36mDataAnalyzer.evaluate_classifiers\u001b[0;34m(self, clf, embedded_data_dict, labels, prompts, test_size, clf_name)\u001b[0m\n\u001b[1;32m    289\u001b[0m     metrics_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    290\u001b[0m     metrics_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_decision_boundary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_dict\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:366\u001b[0m, in \u001b[0;36mDataAnalyzer.plot_decision_boundary\u001b[0;34m(self, clf, representations, labels, prompts, layer, clf_name)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Plots relating to metrics are saved in the metrics directory\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# but maybe they should be saved in the images directory?\u001b[39;00m\n\u001b[1;32m    365\u001b[0m pio\u001b[38;5;241m.\u001b[39mwrite_html(fig, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/decision_boundary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/decision_boundary_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclf_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/basedatatypes.py:3841\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3782\u001b[0m \u001b[38;5;124;03mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03mobject\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/io/_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 )\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Request image\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# ---------\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/io/_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "# Create a button to run the selected visualizations\n",
    "run_button = widgets.Button(description=\"Run Visualizations\")\n",
    "# Assign the event handler to the button\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "\n",
    "# Display the checkboxes and the button\n",
    "checkboxes = widgets.VBox([tsne_checkbox, pca_checkbox, fa_checkbox, raster_checkbox,\n",
    "                           random_proj_checkbox, probe_hidden_states_checkbox, classifier_battery_checkbox, run_button])\n",
    "display(checkboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSNE: 100%|██████████| 12/12 [00:07<00:00,  1.70it/s]\n",
      "PCA: 100%|██████████| 12/12 [00:02<00:00,  4.41it/s]\n",
      "FeatureAgglomeration: 100%|██████████| 12/12 [00:02<00:00,  5.50it/s]\n",
      "Computing Raster Plots: 100%|██████████| 12/12 [01:22<00:00,  6.85s/it]\n",
      "Random projections analysis: 100%|██████████| 12/12 [00:00<00:00, 1242.39it/s]\n",
      "Probing hidden states: 100%|██████████| 12/12 [00:02<00:00,  4.01it/s]\n",
      "Computing logistic_regression:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m data_analyzer\u001b[38;5;241m.\u001b[39mprobe_hidden_states(activations_cache)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See if the representations can be used to classify the ethical area\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Why are we actually doing this? Hypothesis - better seperation of ethical areas\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Leads to better steering vectors. This actually needs to be tested.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Only done with the t-SNE representation but could be done with others (PCA, heirarchical clustering, etc.)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mdata_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_battery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsne_embedded_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsne_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsne_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:239\u001b[0m, in \u001b[0;36mDataAnalyzer.classifier_battery\u001b[0;34m(self, embedded_data_dict, labels, prompts, test_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    238\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     metrics_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_classifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedded_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metrics_dict)\n\u001b[1;32m    241\u001b[0m     metrics_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:292\u001b[0m, in \u001b[0;36mDataAnalyzer.evaluate_classifiers\u001b[0;34m(self, clf, embedded_data_dict, labels, prompts, test_size, clf_name)\u001b[0m\n\u001b[1;32m    289\u001b[0m     metrics_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    290\u001b[0m     metrics_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_decision_boundary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_dict\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/steerllm/data_analyser.py:366\u001b[0m, in \u001b[0;36mDataAnalyzer.plot_decision_boundary\u001b[0;34m(self, clf, representations, labels, prompts, layer, clf_name)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Plots relating to metrics are saved in the metrics directory\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# but maybe they should be saved in the images directory?\u001b[39;00m\n\u001b[1;32m    365\u001b[0m pio\u001b[38;5;241m.\u001b[39mwrite_html(fig, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/decision_boundary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/decision_boundary_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclf_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/basedatatypes.py:3841\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3782\u001b[0m \u001b[38;5;124;03mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03mobject\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/io/_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 )\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Request image\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# ---------\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Steering-LLMs/venv/lib/python3.10/site-packages/plotly/io/_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "data_analyzer = DataAnalyzer(images_dir, metrics_dir, 42)\n",
    "\n",
    "# Get various representations for each layer\n",
    "# and plot them\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_embedded_data_dict, tsne_labels, tsne_prompts = data_analyzer.plot_embeddings(activations_cache, tsne_model)\n",
    "pca_model = PCA(n_components=2, random_state=42)\n",
    "pca_embedded_data_dict, pca_labels, pca_prompts = data_analyzer.plot_embeddings(activations_cache, pca_model)\n",
    "fa_model = FeatureAgglomeration(n_clusters=2)\n",
    "fa_embedded_data_dict, fa_labels, fa_prompts = data_analyzer.plot_embeddings(activations_cache, fa_model)\n",
    "\n",
    "# Further analysis\n",
    "data_analyzer.raster_plot(activations_cache)\n",
    "data_analyzer.random_projections_analysis(activations_cache)\n",
    "data_analyzer.probe_hidden_states(activations_cache)\n",
    "\n",
    "# See if the representations can be used to classify the ethical area\n",
    "# Why are we actually doing this? Hypothesis - better seperation of ethical areas\n",
    "# Leads to better steering vectors. This actually needs to be tested.\n",
    "# Only done with the t-SNE representation but could be done with others (PCA, heirarchical clustering, etc.)\n",
    "data_analyzer.classifier_battery(tsne_embedded_data_dict, tsne_labels, tsne_prompts, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the activations cache if required by the configuration\n",
    "if cfg.write_cache:\n",
    "    model_handler.save_activations_cache(activations_cache, experiment_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
