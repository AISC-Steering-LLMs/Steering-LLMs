{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to use a prompt given to some LLM (\"LLM A\") to generate a dataset of different prompts to be fed to some other LLM (\"LLM B\" - potentially the same as \"LLM A\") for which you have access to the weights. This dataset of prompts will be used to create a \"representation vector\" of a property or concept for LLM B in order to \"steer\" LLM B to have more of that property or concept in its outputs. This Notebook does not cover the creation of representations or steering, only the the creation of a dataset of prompts.\n",
    "\n",
    "The kinds of prompts you want to generate should be about the property or concept you want to steer the LLM towards, not necessary literally mentioning it - e.g. a prompts about politeness do not necessarily need to have the word polite in them. Part of the point of dataset creation is to explore which kinds of generated prompts yield good representations.\n",
    "\n",
    "The Notebook works as follows:\n",
    "\n",
    "- The Setup section loads Python libraries needed to run the code. You do not need to change anything here.\n",
    "- The Inputs section is where you define the prompt you will use to generate your dataset of prompts. Instructions on how to do this are given. This is the only section of the notebook where you will need to change anything.\n",
    "- The Review section then generates a small example dataset of prompts and shows them to you. If you like them, continue on to the end of the Notebook.\n",
    "- If you do not like them, please go back to the Inputs section to refine your prompt to generate your dataset of prompts.\n",
    "- The Dataset Generation section completes the dataset generation\n",
    "- The View Dataset section loads your generated dataset for inspection.\n",
    "- Your dataset will be stored in /data/inputs/name_of_your_dataset/dataset if you want to use it later.\n",
    "\n",
    "The datasets will be generated in CSV format and should have the following form, where the first line is the column headings, the the next two lines are example prompts. The columns ethical_area and ethical_valency are two different labels (classifications) of the prompt. The Notebook will help you generate these labels. This example is for \"politeness\" prompts:\n",
    "\n",
    "```\n",
    "prompt, ethical_area, ethical_valency\n",
    "\"Would you be so kind as to pass the water please.\", \"polite\", 1\n",
    "\"Give me the water now.\", \"impolite\", 0\n",
    "```\n",
    "\n",
    "If this is too restrictive, you are will be able to create other columns and the notebook will ask you about that too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific setup\n",
    "\n",
    "# !git clone https://github.com/AISC-Steering-LLMs/Steering-LLMs\n",
    "# !pwd\n",
    "# repo_path = '/content/repository/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# For refactored code\n",
    "# Need to tidy this up and remove duplicates\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from data_analyser import DataAnalyzer\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "# For datsaet generation\n",
    "import IPython\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import yaml\n",
    "from ipywidgets import widgets, VBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     api_key=\"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\"\n",
    "# )\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Say this is a test\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you might need to alter to change your prompt dataset generation requirements is in this inputs section of the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!CAUTION!!! Enter below your OpenAI API key within the quote marks \" \".\n",
    "\n",
    "- This is not safe practice but will allow the Notebook to run.\n",
    "- Better practice is to store the key in your environment variables. Please ask if you would like help with this.\n",
    "- If you plan to push or share this code in any other way, make sure to remove your API key from this section.\n",
    "\n",
    "```\n",
    "client = OpenAI(\n",
    "    api_key=\"your_api_key_goes_here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI(\n",
    "#    api_key=\"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\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell represents the better practice for using your API key if it is saved in your environment variables. This code is currenty \"commented out\" (has the # symbol in front of each line). If your API key is saved in your environment variables and you want to use this code instead of the code in the previous cell, you will need to remove the # symbol from each line to make it work.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aisc/Steering-LLMs/.venv/lib/python3.10/site-packages/openai/_client.py:98\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     96\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the name of the OpenAI model to use within the quote marks from he list of allowable models in the OpenAI API given your subscription level. This is unlikely to change unless the list of available models is updated. See here for the list of models: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the filename to save the dataset to. Try to give it a specific name e.g. \"honesty_v2.csv\" or \"honesty_pairs_v2.csv\", something that will help you remember what it is especially if you are experimenting with variations. Giving it the same name as an existing dataset will currently overwrite the existing dataset. It should always end in \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"honesty.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the total number of prompts you want to generate in your dataset of prompts. We are about to use a prompt to generate a datset of prompt examples. If you want to generate 10 prompts in total, put 10 here. If you want to generate 1000 prompts in total, put 1000 here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_examples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the number of examples per request. This is the number of prompt examples you want to generate in one request to the LLM.\n",
    "\n",
    "- This is different from the total_num_examples variable you entered above.\n",
    "- The reason we have this is because the LLM has a limit on the number of tokens it can process in one request. If you put a number that is too high here, you will not get the number of prompts you want.\n",
    "- The number of tokens for gpt-4-0125-preview is 4096.\n",
    "- From a quick internet search, I think this model uses about 1.3 tokens per word. So you might have about 4096/1.3 = 3150 words to play with if using this model. Remember, the number of tokens or words includes those in the prompt.\n",
    "- Depending on the size of your prompt and the size of each example you plan to generate, you may need to adjust this number.\n",
    "- This Notebook will make sure that the total number of examples you want to generate as defined in total_num_examples are generated, but it will make multiple requests to the LLM. Eg if you made total_num_examples = 100 and num_examples_per_request = 5, then this code will automatically make 100/5 = 20 requests to the LLM to generate the 100 examples you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_per_request = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below your prompt for generating a dataset prompts.Things to note:\n",
    "- You need to break up your prompt within the square brackets for readability. You simply write as much as you want on one line within quote marks \" \" and then add a comma at the end of the line. Then you can start a new line. The Notebook will reconstruct the prompt as one long string for you, putting spaces between the content of the different lines you have written.\n",
    "- If you want to use quote marks in your prompt, you need to use inverted commas insted. Eg. Use 'this', not \"this\".\n",
    "- You need to add the num_examples_per_request in the weird Python format as shown in the default example below. The curly brackets go where you want the number to go, then at the end of the line with the curly brackets, you put .format(num_examples_per_request) as in:\n",
    "\n",
    "\n",
    "```\n",
    "\"Could you please write {} questions about honesty with exactly\".format(num_examples_per_request)\n",
    "```\n",
    "\n",
    "- You only need to write enough in your prompt to generate new prompt examples. We are not trying to get it to classify the prompt as well at this stage, so you don't need to ask it to the classify the prompt as good or bad or anthying like that at this point. We will get to that.\n",
    "- What we want is to create a single column CSV where the single column is a column of our generated prompt examples.\n",
    "- We therefore need to tell the LLM about the structure of the output we want to make this easier. Complicated prompts such as prompts generating contrasting pairs (e.g. a good and bad version of the same prompt), will require more specific instructions as you can see in the default example below, because we want each prompt of the pair of contrasting prompts on seperate rows of the CSV.\n",
    "- Whatever you do for 4, I recommend you always include the following at the end of your prompt:\n",
    "\n",
    "```\n",
    "\"Do not write any other text in your output except for a correctly formatted CSV\",\n",
    "\"such that your output text could be loaded as a valid CSV.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "        \"Could you please write {} questions about honesty with exactly\".format(num_examples_per_request),\n",
    "        \"the same structure as in my examaple? The questions should be\",\n",
    "        \"asked exactly the same way twice, differing only in the ending.\",\n",
    "        \"The ending is either 'The answer is A because' or 'The answer is B because'\",\n",
    "        \"Here is my example:\",\n",
    "        \"How should you approach honesty in your daily life?\",\n",
    "        \"Select from the following options:\",\n",
    "        \"A. Always be truthful and transparent in your interactions.\",\n",
    "        \"B. Definitely by lying and deceiving whenever it benefits you!\",\n",
    "        \"The answer is A because\",\n",
    "        \"How should you approach honesty in your daily life?\",\n",
    "        \"Select from the following options:\",\n",
    "        \"A. Always be truthful and transparent in your interactions.\",\n",
    "        \"B. Definitely by lying and deceiving whenever it benefits you!\",\n",
    "        \"The answer is B because\",\n",
    "        \"End of example.\",\n",
    "        \"Format your output as a single column CSV.\",\n",
    "        \"Each row of the CSV is a single question in quote marks followed by a comma,\",\n",
    "        \"so each question in a pair of identical questions differing only in the ending is on a seperate row.\",\n",
    "        \"Do not write any other text in your output except for a correctly formatted CSV\",\n",
    "        \"such that your output text could be loaded as a valid CSV.\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Could you please write 5 questions about honesty with exactly',\n",
       " 'the same structure as in my examaple? The questions should be',\n",
       " 'asked exactly the same way twice, differing only in the ending.',\n",
       " \"The ending is either 'The answer is A because' or 'The answer is B because'\",\n",
       " 'Here is my example:',\n",
       " 'How should you approach honesty in your daily life?',\n",
       " 'Select from the following options:',\n",
       " 'A. Always be truthful and transparent in your interactions.',\n",
       " 'B. Definitely by lying and deceiving whenever it benefits you!',\n",
       " 'The answer is A because',\n",
       " 'How should you approach honesty in your daily life?',\n",
       " 'Select from the following options:',\n",
       " 'A. Always be truthful and transparent in your interactions.',\n",
       " 'B. Definitely by lying and deceiving whenever it benefits you!',\n",
       " 'The answer is B because',\n",
       " 'End of example.',\n",
       " 'Format your output as a single column CSV.',\n",
       " 'Each row of the CSV is a single question in quote marks followed by a comma,',\n",
       " 'so each question in a pair of identical questions differing only in the ending is on a seperate row.',\n",
       " 'Do not write any other text in your output except for a correctly formatted CSV',\n",
       " 'such that your output text could be loaded as a valid CSV.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0125-preview\"\n",
    "prompt_structure_dir = \"pairs_v1\"\n",
    "template_file = \"template_multi.j2\"\n",
    "prompt_context_file = \"honesty.json\"\n",
    "num_examples_per_prompt = \"1\" # Must be a whole number inside quote marks. Max is 75.\n",
    "total_num_examples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SRC_PATH = \"../data/inputs\"\n",
    "DATASET_BUILDER_DIR_PATH = os.path.join(SRC_PATH, \"prompts\", prompt_structure_dir)\n",
    "\n",
    "# Number of interations of the prompt to generate the entire dataset\n",
    "num_iterations = math.ceil(total_num_examples/int(num_examples_per_prompt))\n",
    "\n",
    "# Input directories and files\n",
    "template_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"templates\", template_file)\n",
    "prompt_context, _ = os.path.splitext(prompt_context_file)\n",
    "prompt_context_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"contexts\", prompt_context+\".json\")\n",
    "\n",
    "# Output directories and files\n",
    "dataset_generator_prompt_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"dataset_generator_prompts\", prompt_context+\"_prompt.txt\")\n",
    "generated_dataset_dir = os.path.join(DATASET_BUILDER_DIR_PATH, \"generated_datasets\")\n",
    "generated_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_dataset\")\n",
    "log_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"logs\", prompt_context+\"_log\")\n",
    "combined_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the prompt from the template and template material\n",
    "def render_template_with_data(template_file_path,\n",
    "                              prompt_context_file_path,\n",
    "                              dataset_generator_prompt_file_path,\n",
    "                              num_examples_per_prompt):\n",
    "\n",
    "    # Set up the environment with the path to the directory containing the template\n",
    "    env = Environment(loader=FileSystemLoader(os.path.dirname(template_file_path)))\n",
    "\n",
    "    # Now, get_template should be called with the filename only, not the path\n",
    "    template = env.get_template(os.path.basename(template_file_path))\n",
    "    \n",
    "    # Load the prompt context\n",
    "    with open(prompt_context_file_path, 'r') as file:\n",
    "        prompt_construction_options = json.load(file)\n",
    "\n",
    "    # Update the prompt context example to replace the list with the combined string\n",
    "    example_text = \"\\n\".join(prompt_construction_options[\"example\"])\n",
    "    prompt_construction_options[\"example\"] = example_text\n",
    "    prompt_construction_options[\"num_examples\"] = num_examples_per_prompt\n",
    "\n",
    "    # Render the template with the prompt_construction_options\n",
    "    prompt_to_generate_dataset = template.render(prompt_construction_options)\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    # Remove newlines from the prompt and replace with spaces\n",
    "    prompt_to_generate_dataset = prompt_to_generate_dataset.replace('\\n', ' ')\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    return prompt_to_generate_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Generate the dataset by calling the OpenAI API\n",
    "def generate_dataset_from_prompt(prompt,\n",
    "                                 generated_dataset_file_path,\n",
    "                                 model,\n",
    "                                 log_file_path,\n",
    "                                 i):\n",
    "    completion = client.chat.completions.create(\n",
    "            **{\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    completion_words = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\n",
    "    print(\" \")\n",
    "    print(completion_words)\n",
    "    print(\" \")\n",
    "\n",
    "    # Open a file in write mode ('w') and save the CSV data\n",
    "    with open(generated_dataset_file_path+\"_\"+str(i)+\".txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "        file.write(completion_words)\n",
    "\n",
    "    num_words_in_prompt = count_words_in_string(prompt)\n",
    "    num_words_in_completion = count_words_in_string(completion_words)\n",
    "    total_words = num_words_in_prompt + num_words_in_completion\n",
    "\n",
    "    num_tokens_in_prompt = completion.usage.prompt_tokens\n",
    "    num_tokens_in_completion = completion.usage.completion_tokens\n",
    "    total_tokens = num_tokens_in_prompt + num_tokens_in_completion\n",
    "\n",
    "    prompt_cost = num_tokens_in_prompt*0.01/1000\n",
    "    completion_cost = num_tokens_in_completion*0.03/1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    tokens_per_prompt_word = num_words_in_prompt/num_tokens_in_prompt\n",
    "    tokens_per_completion_word = num_words_in_completion/num_tokens_in_completion\n",
    "\n",
    "    log = {\n",
    "            \"num_words_in_prompt\": num_words_in_prompt,\n",
    "            \"num_words_in_completion\": num_words_in_completion,\n",
    "            \"total_words\": total_words,\n",
    "            \"num_tokens_in_prompt\": num_tokens_in_prompt,\n",
    "            \"num_tokens_in_completion\": num_tokens_in_completion,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"prompt_cost\": prompt_cost,\n",
    "            \"completion_cost\": completion_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"tokens_per_prompt_word\": tokens_per_prompt_word,\n",
    "            \"tokens_per_completion_word\": tokens_per_completion_word\n",
    "\n",
    "    }\n",
    "\n",
    "    for k, v in log.items():\n",
    "        print(k, v)\n",
    "    print(\" \")\n",
    "\n",
    "    with open(log_file_path+\"_\"+str(i)+\".txt\", 'w') as file:\n",
    "        file.write(json.dumps(log, indent=4))\n",
    "\n",
    "def count_words_in_string(input_string):\n",
    "    words = input_string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mgenerate_dataset_from_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_dataset_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mgenerate_dataset_from_prompt\u001b[0;34m(prompt, generated_dataset_file_path, model, log_file_path, i)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_dataset_from_prompt\u001b[39m(prompt,\n\u001b[1;32m     42\u001b[0m                                  generated_dataset_file_path,\n\u001b[1;32m     43\u001b[0m                                  model,\n\u001b[1;32m     44\u001b[0m                                  log_file_path,\n\u001b[1;32m     45\u001b[0m                                  i):\n\u001b[0;32m---> 46\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     48\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m     49\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     50\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     51\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     52\u001b[0m                 ]\n\u001b[1;32m     53\u001b[0m             }\n\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     completion_words \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = render_template_with_data(template_file_path,\n",
    "                                   prompt_context_file_path,\n",
    "                                   dataset_generator_prompt_file_path,\n",
    "                                   num_examples_per_prompt,\n",
    "                                   )\n",
    "\n",
    "# Generate the dataset\n",
    "for i in range(num_iterations):\n",
    "    print(\"Iteration: \", i)\n",
    "    generate_dataset_from_prompt(prompt, generated_dataset_file_path, model, log_file_path, i)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files you want to process\n",
    "# Makes sure they all have the same prompt context\n",
    "# eg won;t mix up honest with justice etc\n",
    "files = [os.path.join(generated_dataset_dir, f) for f in os.listdir(generated_dataset_dir) if f.endswith('.txt') and prompt_context in f]\n",
    "\n",
    "# Define the regular expression pattern\n",
    "# Get lines that start with a quote,\n",
    "# then have any number of characters,\n",
    "# then end with a quote and possibly comma\n",
    "# We're trying to find all valid CSV lines\n",
    "pattern = r'^\\\".*\\\",?[\\r\\n]*'\n",
    "\n",
    "# Open the master CSV file\n",
    "with open(combined_dataset_file_path, \"a\") as master:\n",
    "    # Loop over the files\n",
    "    for file in files:\n",
    "        # Open the current file and read its contents\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use the re.findall function to find all matches in the content\n",
    "        matches = re.findall(pattern, content, re.MULTILINE)        \n",
    "\n",
    "        # Loop over the matches\n",
    "        for match in matches:\n",
    "            \n",
    "            # Remove any trailing commas and newline characters\n",
    "            match_cleaned = match.rstrip(',\\r\\n')\n",
    "            \n",
    "            # Append the match to the master CSV file\n",
    "            master.write(match_cleaned + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optional columns for classification\n",
    "\n",
    "The columns added here work with the defaults currently hardcoded into the data analysis\n",
    "This hardcoding will be resolved soon for more flexibility.\n",
    "\n",
    "I show examples of using am LLM to auto-label and a way to label if you knw in advance which rows are of which kind of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "input_file_path = combined_dataset_file_path\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Add a header\n",
    "    writer.writerow([\"Prompt\", \"Ethical Area\"])\n",
    "    \n",
    "    for row in reader:\n",
    "        # Assuming each row contains a single column with your text\n",
    "        question = row[0]  # Adjust this if your structure is different\n",
    "        # Here you define the question you want to ask about each row\n",
    "        prompt = f\"Do you think the start of the response in '{question}' is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks.\"\n",
    "        response = ask_openai(prompt)\n",
    "        # Add the OpenAI response to the row\n",
    "        row.append(response)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling programatically without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we might not need to query the API for kind of lebelling\n",
    "# Eg if we know the questions always go good, bad, good, bad, good etc\n",
    "\n",
    "input_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_fully_labelled.csv\")\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # If your CSV has a header and you want to keep it, read and write it first\n",
    "    # This also allows you to add a new column name to the header\n",
    "    header = next(reader)\n",
    "    header.append(\"Positive\")  # Add your new column name here\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Enumerate adds a counter to an iterable and returns it (the enumerate object).\n",
    "    for index, row in enumerate(reader, start=1):  # Start counting from 1\n",
    "        if index % 2 == 0:  # Check if the row number is even\n",
    "            row.append(0)\n",
    "        else:\n",
    "            row.append(1)\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
