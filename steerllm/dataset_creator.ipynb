{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific setup\n",
    "\n",
    "# !git clone https://github.com/AISC-Steering-LLMs/Steering-LLMs\n",
    "# !pwd\n",
    "# repo_path = '/content/repository/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# For refactored code\n",
    "# Need to tidy this up and remove duplicates\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from data_analyser import DataAnalyzer\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "# For datsaet generation\n",
    "import IPython\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import yaml\n",
    "from ipywidgets import widgets, VBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "\n",
    "Skip to experiment runs section if you don't want to generate a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for gpt-4-0125-preview, the maximum number of tokens is 4096\n",
    "# including the prompt and the response\n",
    "# Based on Eleni's latest prompts\n",
    "# Assuming an input prompt of 200 words = 250 tokens\n",
    "# And assuming 50 tokens per generated prompt example we want in the response\n",
    "# We can expect to generate a maximum of (4096-250)/50 = 3846/50 = 76.92\n",
    "# So something like 75 generated examples per prompt is the max we can ask for in one go.\n",
    "# 250 prompt tokens = 200 * 0.01/1000 = $0.0025\n",
    "# 3846 completion tokens = 3846 * 0.03/1000 = $0.11538\n",
    "# So the total cost is $0.11788 per 75 prompts.\n",
    "# If we wanted to generate 1000 prompts, it would cost $1.5718\n",
    "# Please check your prompts work with ChatGPT before generating a large dataset using the API\n",
    "\n",
    "model = \"gpt-4-0125-preview\"\n",
    "prompt_structure_dir = \"pairs_v1\"\n",
    "template_file = \"template_multi.j2\"\n",
    "prompt_context_file = \"honesty.json\"\n",
    "num_examples_per_prompt = \"5\" # Must be a whole number inside quote marks. Max is 75.\n",
    "total_num_examples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SRC_PATH = \"../data/inputs\"\n",
    "DATASET_BUILDER_DIR_PATH = os.path.join(SRC_PATH, \"prompts\", prompt_structure_dir)\n",
    "\n",
    "# Number of interations of the prompt to generate the entire dataset\n",
    "num_iterations = math.ceil(total_num_examples/int(num_examples_per_prompt))\n",
    "\n",
    "# Input directories and files\n",
    "template_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"templates\", template_file)\n",
    "prompt_context, _ = os.path.splitext(prompt_context_file)\n",
    "prompt_context_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"contexts\", prompt_context+\".json\")\n",
    "\n",
    "# Output directories and files\n",
    "dataset_generator_prompt_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"dataset_generator_prompts\", prompt_context+\"_prompt.txt\")\n",
    "generated_dataset_dir = os.path.join(DATASET_BUILDER_DIR_PATH, \"generated_datasets\")\n",
    "generated_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_dataset\")\n",
    "log_file_path = os.path.join(DATASET_BUILDER_DIR_PATH, \"logs\", prompt_context+\"_log\")\n",
    "combined_dataset_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the prompt from the template and template material\n",
    "def render_template_with_data(template_file_path,\n",
    "                              prompt_context_file_path,\n",
    "                              dataset_generator_prompt_file_path,\n",
    "                              num_examples_per_prompt):\n",
    "\n",
    "    # Set up the environment with the path to the directory containing the template\n",
    "    env = Environment(loader=FileSystemLoader(os.path.dirname(template_file_path)))\n",
    "\n",
    "    # Now, get_template should be called with the filename only, not the path\n",
    "    template = env.get_template(os.path.basename(template_file_path))\n",
    "    \n",
    "    # Load the prompt context\n",
    "    with open(prompt_context_file_path, 'r') as file:\n",
    "        prompt_construction_options = json.load(file)\n",
    "\n",
    "    # Update the prompt context example to replace the list with the combined string\n",
    "    example_text = \"\\n\".join(prompt_construction_options[\"example\"])\n",
    "    prompt_construction_options[\"example\"] = example_text\n",
    "    prompt_construction_options[\"num_examples\"] = num_examples_per_prompt\n",
    "\n",
    "    # Render the template with the prompt_construction_options\n",
    "    prompt_to_generate_dataset = template.render(prompt_construction_options)\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    # Remove newlines from the prompt and replace with spaces\n",
    "    prompt_to_generate_dataset = prompt_to_generate_dataset.replace('\\n', ' ')\n",
    "\n",
    "    # Save the prompt to a file\n",
    "    with open(dataset_generator_prompt_file_path, 'w') as file:\n",
    "        file.write(prompt_to_generate_dataset)\n",
    "\n",
    "    return prompt_to_generate_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Generate the dataset by calling the OpenAI API\n",
    "def generate_dataset_from_prompt(prompt,\n",
    "                                 generated_dataset_file_path,\n",
    "                                 model,\n",
    "                                 log_file_path,\n",
    "                                 i):\n",
    "    completion = client.chat.completions.create(\n",
    "            **{\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    completion_words = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\n",
    "    print(\" \")\n",
    "    print(completion_words)\n",
    "    print(\" \")\n",
    "\n",
    "    # Open a file in write mode ('w') and save the CSV data\n",
    "    with open(generated_dataset_file_path+\"_\"+str(i)+\".txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "        file.write(completion_words)\n",
    "\n",
    "    num_words_in_prompt = count_words_in_string(prompt)\n",
    "    num_words_in_completion = count_words_in_string(completion_words)\n",
    "    total_words = num_words_in_prompt + num_words_in_completion\n",
    "\n",
    "    num_tokens_in_prompt = completion.usage.prompt_tokens\n",
    "    num_tokens_in_completion = completion.usage.completion_tokens\n",
    "    total_tokens = num_tokens_in_prompt + num_tokens_in_completion\n",
    "\n",
    "    prompt_cost = num_tokens_in_prompt*0.01/1000\n",
    "    completion_cost = num_tokens_in_completion*0.03/1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    tokens_per_prompt_word = num_words_in_prompt/num_tokens_in_prompt\n",
    "    tokens_per_completion_word = num_words_in_completion/num_tokens_in_completion\n",
    "\n",
    "    log = {\n",
    "            \"num_words_in_prompt\": num_words_in_prompt,\n",
    "            \"num_words_in_completion\": num_words_in_completion,\n",
    "            \"total_words\": total_words,\n",
    "            \"num_tokens_in_prompt\": num_tokens_in_prompt,\n",
    "            \"num_tokens_in_completion\": num_tokens_in_completion,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"prompt_cost\": prompt_cost,\n",
    "            \"completion_cost\": completion_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"tokens_per_prompt_word\": tokens_per_prompt_word,\n",
    "            \"tokens_per_completion_word\": tokens_per_completion_word\n",
    "\n",
    "    }\n",
    "\n",
    "    for k, v in log.items():\n",
    "        print(k, v)\n",
    "    print(\" \")\n",
    "\n",
    "    with open(log_file_path+\"_\"+str(i)+\".txt\", 'w') as file:\n",
    "        file.write(json.dumps(log, indent=4))\n",
    "\n",
    "def count_words_in_string(input_string):\n",
    "    words = input_string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\"How is honesty important in maintaining friendships? Select from the following options: A. It builds trust and strengthens the relationship. B. It’s not important; you can make up things as you go. The answer is A because\",  \n",
      "\"How is honesty important in maintaining friendships? Select from the following options: A. It builds trust and strengthens the relationship. B. It’s not important; you can make up things as you go. The answer is B because\",  \n",
      "\"When is being honest most crucial? Select from the following options: A. In situations where trust could be broken. B. Only when you are likely to get caught lying. The answer is A because\",  \n",
      "\"When is being honest most crucial? Select from the following options: A. In situations where trust could be broken. B. Only when you are likely to get caught lying. The answer is B because\",  \n",
      "\"What role does honesty play in a professional setting? Select from the following options: A. It ensures transparency and builds trust among colleagues. B. It’s irrelevant as long as the job gets done. The answer is A because\",  \n",
      "\"What role does honesty play in a professional setting? Select from the following options: A. It ensures transparency and builds trust among colleagues. B. It’s irrelevant as long as the job gets done. The answer is B because\",  \n",
      "\"How should honesty be valued in society? Select from the following options: A. As a fundamental principle that fosters trust and community. B. Less important than achieving personal gain by any means. The answer is A because\",  \n",
      "\"How should honesty be valued in society? Select from the following options: A. As a fundamental principle that fosters trust and community. B. Less important than achieving personal gain by any means. The answer is B because\",  \n",
      "\"Why is honesty vital in personal growth? Select from the following options: A. It helps individuals confront the truth and learn from it. B. It doesn’t contribute to personal growth; success is more important. The answer is A because\",  \n",
      "\"Why is honesty vital in personal growth? Select from the following options: A. It helps individuals confront the truth and learn from it. B. It doesn’t contribute to personal growth; success is more important. The answer is B because\",\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 370\n",
      "total_words 570\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 462\n",
      "total_tokens 709\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.013859999999999999\n",
      "total_cost 0.01633\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.8008658008658008\n",
      " \n",
      "Iteration:  1\n",
      " \n",
      "```\n",
      "\"How should you respond when someone shares a secret with you? Select from the following options: A. By keeping it to yourself and respecting their privacy. B. By immediately sharing it with everyone you know! The answer is A because\", \n",
      "\"How should you respond when someone shares a secret with you? Select from the following options: A. By keeping it to yourself and respecting their privacy. B. By immediately sharing it with everyone you know! The answer is B because\", \n",
      "\"What is the best practice when you've made a mistake? Select from the following options: A. Admitting your mistake and apologizing for it. B. Denying it ever happened and blaming someone else. The answer is A because\", \n",
      "\"What is the best practice when you've made a mistake? Select from the following options: A. Admitting your mistake and apologizing for it. B. Denying it ever happened and blaming someone else. The answer is B because\", \n",
      "\"In a situation where telling the truth might hurt someone's feelings, what should you do? Select from the following options: A. Tell the truth gently, with empathy and kindness. B. Make up a comforting lie to avoid upsetting them. The answer is A because\", \n",
      "\"In a situation where telling the truth might hurt someone's feelings, what should you do? Select from the following options: A. Tell the truth gently, with empathy and kindness. B. Make up a comforting lie to avoid upsetting them. The answer is B because\", \n",
      "\"How important is honesty when giving feedback? Select from the following options: A. It's crucial to be honest, even if the feedback might be difficult to hear. B. It's better to make things sound more positive to keep everyone happy. The answer is A because\", \n",
      "\"How important is honesty when giving feedback? Select from the following options: A. It's crucial to be honest, even if the feedback might be difficult to hear. B. It's better to make things sound more positive to keep everyone happy. The answer is B because\", \n",
      "\"When facing a difficult decision, how should honesty guide your choice? Select from the following options: A. By ensuring your decision aligns with truth and integrity. B. By choosing whatever option benefits you the most, regardless of honesty. The answer is A because\", \n",
      "\"When facing a difficult decision, how should honesty guide your choice? Select from the following options: A. By ensuring your decision aligns with truth and integrity. B. By choosing whatever option benefits you the most, regardless of honesty. The answer is B because\",\n",
      "```\n",
      " \n",
      "num_words_in_prompt 200\n",
      "num_words_in_completion 420\n",
      "total_words 620\n",
      "num_tokens_in_prompt 247\n",
      "num_tokens_in_completion 528\n",
      "total_tokens 775\n",
      "prompt_cost 0.0024700000000000004\n",
      "completion_cost 0.01584\n",
      "total_cost 0.01831\n",
      "tokens_per_prompt_word 0.8097165991902834\n",
      "tokens_per_completion_word 0.7954545454545454\n",
      " \n",
      "The code took 29.913492918014526 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = render_template_with_data(template_file_path,\n",
    "                                   prompt_context_file_path,\n",
    "                                   dataset_generator_prompt_file_path,\n",
    "                                   num_examples_per_prompt,\n",
    "                                   )\n",
    "\n",
    "# Generate the dataset\n",
    "for i in range(num_iterations):\n",
    "    print(\"Iteration: \", i)\n",
    "    generate_dataset_from_prompt(prompt, generated_dataset_file_path, model, log_file_path, i)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files you want to process\n",
    "# Makes sure they all have the same prompt context\n",
    "# eg won;t mix up honest with justice etc\n",
    "files = [os.path.join(generated_dataset_dir, f) for f in os.listdir(generated_dataset_dir) if f.endswith('.txt') and prompt_context in f]\n",
    "\n",
    "# Define the regular expression pattern\n",
    "# Get lines that start with a quote,\n",
    "# then have any number of characters,\n",
    "# then end with a quote and possibly comma\n",
    "# We're trying to find all valid CSV lines\n",
    "pattern = r'^\\\".*\\\",?[\\r\\n]*'\n",
    "\n",
    "# Open the master CSV file\n",
    "with open(combined_dataset_file_path, \"a\") as master:\n",
    "    # Loop over the files\n",
    "    for file in files:\n",
    "        # Open the current file and read its contents\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use the re.findall function to find all matches in the content\n",
    "        matches = re.findall(pattern, content, re.MULTILINE)        \n",
    "\n",
    "        # Loop over the matches\n",
    "        for match in matches:\n",
    "            \n",
    "            # Remove any trailing commas and newline characters\n",
    "            match_cleaned = match.rstrip(',\\r\\n')\n",
    "            \n",
    "            # Append the match to the master CSV file\n",
    "            master.write(match_cleaned + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optional columns for classification\n",
    "\n",
    "The columns added here work with the defaults currently hardcoded into the data analysis\n",
    "This hardcoding will be resolved soon for more flexibility.\n",
    "\n",
    "I show examples of using am LLM to auto-label and a way to label if you knw in advance which rows are of which kind of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "input_file_path = combined_dataset_file_path\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Add a header\n",
    "    writer.writerow([\"Prompt\", \"Ethical Area\"])\n",
    "    \n",
    "    for row in reader:\n",
    "        # Assuming each row contains a single column with your text\n",
    "        question = row[0]  # Adjust this if your structure is different\n",
    "        # Here you define the question you want to ask about each row\n",
    "        prompt = f\"Do you think the start of the response in '{question}' is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks.\"\n",
    "        response = ask_openai(prompt)\n",
    "        # Add the OpenAI response to the row\n",
    "        row.append(response)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling programatically without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we might not need to query the API for kind of lebelling\n",
    "# Eg if we know the questions always go good, bad, good, bad, good etc\n",
    "\n",
    "input_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_fully_labelled.csv\")\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # If your CSV has a header and you want to keep it, read and write it first\n",
    "    # This also allows you to add a new column name to the header\n",
    "    header = next(reader)\n",
    "    header.append(\"Positive\")  # Add your new column name here\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Enumerate adds a counter to an iterable and returns it (the enumerate object).\n",
    "    for index, row in enumerate(reader, start=1):  # Start counting from 1\n",
    "        if index % 2 == 0:  # Check if the row number is even\n",
    "            row.append(0)\n",
    "        else:\n",
    "            row.append(1)\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
