{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "\n",
    "# Sort the formatting issue with the Use Template button\n",
    "\n",
    "# Sort the spacing when entering placeholders\n",
    "\n",
    "# Sort the placeholder ordering to reflect the order in the template\n",
    "\n",
    "# Put all code in a class\n",
    "\n",
    "# Have working on Colab\n",
    "\n",
    "# Don't allow saving as .txt.j2 etc\n",
    "\n",
    "# Prompt novelty\n",
    "# random seed but record seeds\n",
    "# length\n",
    "# Sample from top 5?\n",
    "# higher shot in k-shot\n",
    "# bigger model - more expensive\n",
    "\n",
    "# Pick up from where we left off of API call fails\n",
    "# Do you loose everything if you stop the API call?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to use a prompt given to some LLM (\"LLM A\") to generate a dataset of different prompts to be fed to some other LLM (\"LLM B\" - potentially the same as \"LLM A\") for which you have access to the weights. This dataset of prompts will be used to create a \"representation vector\" of a property or concept for LLM B in order to \"steer\" LLM B to have more of that property or concept in its outputs. This Notebook does not cover the creation of representations or steering, only the the creation of a dataset of prompts.\n",
    "\n",
    "The kinds of prompts you want to generate should be about the property or concept you want to steer the LLM towards, not necessary literally mentioning it - e.g. prompts about politeness do not necessarily need to have the word polite in them. Part of the point of dataset creation is to explore which kinds of generated prompts yield good representations.\n",
    "\n",
    "The Notebook works as follows:\n",
    "\n",
    "- The Setup section loads Python libraries needed to run the code. You do not need to change anything here.\n",
    "- The Inputs section is where you define the prompt you will use to generate your dataset of prompts. Instructions on how to do this are given. This is the only section of the notebook where you will need to change anything.\n",
    "- The Review section then generates a small example dataset of prompts and shows them to you. If you like them, continue on to the end of the Notebook.\n",
    "- If you do not like them, please go back to the Inputs section to refine your prompt to generate your dataset of prompts.\n",
    "- The Dataset Generation section completes the dataset generation.\n",
    "- The View Dataset section loads your generated dataset for inspection.\n",
    "- Your dataset will be stored in /data/inputs/name_of_your_dataset/dataset if you want to use it later.\n",
    "\n",
    "The datasets will be generated in CSV format and should have the following form, where the first line is the column headings and subsequent lines are for example prompts. The columns ethical_area and ethical_valence are two different labels (classifications) of the prompt. The Notebook will help you generate these labels. This example is for \"politeness\" prompts:\n",
    "\n",
    "```\n",
    "prompt, ethical_area, ethical_valence\n",
    "\"Would you be so kind as to pass the water please.\", \"polite\", 1\n",
    "\"Give me the water now.\", \"impolite\", 0\n",
    "```\n",
    "\n",
    "If this is too restrictive, you will be able to create other columns and the notebook will ask you about that too.\n",
    "\n",
    "Note that where we have text, such as for the prompt or ethical_area, we want it enclosed in quote marks. Lone numbers do not need this.\n",
    "\n",
    "If there is something unusual you want to do that the current notebook does not permist, please ask, or feel free to try adding code for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "#import main\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import yaml\n",
    "from hydra import initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.experimental import compose\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# For refactored code\n",
    "# Need to tidy this up and remove duplicates\n",
    "\n",
    "# from data_handler import DataHandler\n",
    "# from data_analyser import DataAnalyzer\n",
    "# from model_handler import ModelHandler\n",
    "\n",
    "#from sklearn.manifold import TSNE\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "# For datsaet generation\n",
    "import IPython\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import yaml\n",
    "from ipywidgets import widgets, VBox, Button, Checkbox, Text, IntText, FloatText, SelectMultiple, Label\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you might need to alter to change your prompt dataset generation requirements is in this inputs section of the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!CAUTION!!! Enter below your OpenAI API key within the quote marks \" \".\n",
    "\n",
    "- This is not safe practice but will allow the Notebook to run.\n",
    "- Better practice is to store the key in your environment variables. Please ask if you would like help with this.\n",
    "- If you plan to push or share this code in any other way, make sure to remove your API key from this section.\n",
    "\n",
    "```\n",
    "client = OpenAI(\n",
    "    api_key=\"your_api_key_goes_here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "   api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell represents the better practice for using your API key if it is saved in your environment variables.\n",
    "\n",
    "- This code is currenty \"commented out\" (has the # symbol in front of each line). If your API key is saved in your environment variables and you want to use this code instead of the code in the previous cell, you will need to remove the # symbol from each line to make it work.\n",
    "- There will be nothing to add to this code beyond removing the # symbols.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the name of the OpenAI model to use within the quote marks.\n",
    "\n",
    "- Select the model from the list of allowable models in the OpenAI API given your subscription level (create an account and get an API ket here: https://auth0.openai.com/u/signup/identifier?state=hKFo2SA1REJ3dGFZVTllNHFvYUFkY2RrWEJpUUVMUWxvel91VqFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIFg1Z2NKOU9hUk4yYUFmWGxyTHlscmtNTmMxbDF5dWZTo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q).\n",
    "- You will need to put some token amount of money on to use gpt-4-0125-preview. It's something like $1. Might be $10.\n",
    "- This is unlikely to change unless the list of available models is updated. See here for the list of models: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the model temperature. \n",
    "\n",
    "- From the OpenAI API documentation (https://platform.openai.com/docs/guides/text-generation/how-should-i-set-the-temperature-parameter): \"Lower values for temperature result in more consistent outputs (e.g. 0.2), while higher values generate more diverse and creative results (e.g. 1.0). Select a temperature value based on the desired trade-off between coherence and creativity for your specific application. The temperature can range is from 0 to 2.\"\n",
    "- If your generated sentences are too similar/boring, try increasing the number. Go the other way if too wacky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the filename (or \"filestem\" as written below) without the file extension (e.g. .csv) to save the dataset to.\n",
    "\n",
    "- Don't worry about the file ending. It will be a csv. If you call your filename \"honesty\", then your dataset will be in honesty.csv.\n",
    "- Try to give it a specific name e.g. \"honesty_v2.csv\" or \"honesty_pairs_v2.csv\", something that will help you remember what it is especially if you are experimenting with variations.\n",
    "- Giving it the same name as an existing dataset will currently overwrite the existing dataset. It should always end in \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filestem = \"honesty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the total number of prompts you want to generate in your dataset of prompts.\n",
    "\n",
    "- We are about to use a prompt to generate a datset of prompt examples.\n",
    "- If you want to generate 10 prompts in total, put 10 here.\n",
    "- If you want to generate 1000 prompts in total, put 1000 here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_examples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below the number of examples per request. This is the number of prompt examples you want to generate in one request to the LLM.\n",
    "\n",
    "- This is different from the total_num_examples variable you entered above.\n",
    "- The reason we have this is because the LLM has a limit on the number of tokens it can process in one request. If you put a number that is too high here, you will not get the number of prompts you want.\n",
    "- The number of tokens for gpt-4-0125-preview is 128,000 based on details given here: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo.\n",
    "- From a quick internet search, I think this model uses about 1.3 tokens per word. So you might have about 4096/1.3 = 98,461 words to play with if using this model. Remember, the number of tokens or words includes those in the prompt.\n",
    "- This should be more than enough to generate your entire dataset in one go, so you could try setting num_examples_per_request to the same number as total_num_examples. One reason to err on slightly fewer examples is that bigger responses from OpenAI take longer to generate. If you lose your internet connection before generation has ended, you lose your entire generated dataset. If you generate the data in small batches, you will have a partially complete dataset up until the batch that fails.\n",
    "- This Notebook will make sure that the total number of examples you want to generate as defined in total_num_examples are generated, but it will make multiple requests to the LLM. Eg if you made total_num_examples = 100 and num_examples_per_request = 5, then this code will automatically make 100/5 = 20 requests to the LLM to generate the 100 examples you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_per_request = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use or modify an existing templete, or to create a new one, run all the cells below up to and including the one with the code Initialize() in it.\n",
    "\n",
    "What is a template?\n",
    "- A generic prompt stucture.\n",
    "- Any variable/placeholder you might want to pass to your template should be include within a pair of braces like so: {{ your_variable_name }}\n",
    "\n",
    "To create a new template (this could be better):\n",
    "- Use the default blank_template.j2\n",
    "- Enter the template structure in the text box below the dropdown.\n",
    "- Create a new filename and click save\n",
    "- You cannot name a file blank_template.j2 and will be prompted to create a new file.\n",
    "- You can overwirte other existing template filenames but will recieve a warning beforehand that you are trying to overwrite and existing file. It is recommended that you always use a new filename for your new template for the sake of tracking experiments.\n",
    "- You can enter any filename you like with anyfile ending, but the file will always be saved as a .j2 file. E.g. if you save the file as politeness_template.md, it will get saved as politeness_template.txt. If you don;t supply a file ending, .txt will also be added.\n",
    "- To be able to use this saved template with a new name, you will need to make sure it is selected in the original dropdown menu at the top of the form. E.g. if you started with blank_template.j2 in the dropdown, created a new template and saved it as politeness_template.txt, you will need to go back to the top dropdown menu and select the now present politeness_template.txt.\n",
    "- You can now clock the Use Template Button.\n",
    "- Note: you can also create a new template using the template modification steps outlined next.\n",
    "\n",
    "To modify an existing template:\n",
    "- Select the desired template from the dropdown menu.\n",
    "- Clicking on the template will load it into the the text box below the dropdown.\n",
    "- Modify as needed.\n",
    "- The same comments about saving apply as to creating a new template mentioned above.\n",
    "- The same comments about using the template apply as to creating a new template mentioned above.\n",
    "\n",
    "To use an existing template without modification:\n",
    "- Select the desired template from the dropdown menu.\n",
    "- Do not change any text loaded into the the text box below the dropdown.\n",
    "- You do not need to save the existing template.\n",
    "- Click directly on the Use Template button.\n",
    "\n",
    "With the Use Template button clicked:\n",
    "- You will be asked to enter specific values for any variable/placeholder you have in your template.\n",
    "- This turns your prompt template into a specific prompt.\n",
    "- You will also be asked to enter a new filename to save the prompt.\n",
    "- The same saving rules apply to prompts as to templates described above, only the code forces saving the prompt as a .txt and not a .js.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to work with an existing template and not load it, ignore the last cell and run this one.\n",
    "\n",
    "- Select the template you want.\n",
    "- You will be prompted to enter values for your variables/placeholders.\n",
    "- You will also be asked to save this prompt (where templates and prompts are saved and how they can be related needs to \n",
    "be thought about).\n",
    "- Create a new filename and click save.\n",
    "- You cannot overwirte existing prompts. New prompts have to be saved with new names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ipywidgets import Dropdown, Textarea, Button, VBox, Label, Text, Output, HBox, Layout, widgets\n",
    "from IPython.display import display, HTML\n",
    "from jinja2 import Environment, FileSystemLoader, meta\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .widget-label {\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        overflow-wrap: break-word;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('''\n",
    "<style>\n",
    "    .widget-label {\n",
    "        white-space: normal;\n",
    "        word-wrap: break-word;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Jinja environment\n",
    "template_dir = '../data/inputs/templates'\n",
    "output_dir = '../data/inputs/prompts'\n",
    "env = Environment(loader=FileSystemLoader(template_dir))\n",
    "\n",
    "def load_templates():\n",
    "    return [f for f in os.listdir(template_dir) if f.endswith(('.j2'))]\n",
    "\n",
    "def create_template_dropdown():\n",
    "    templates = load_templates()\n",
    "    default_template = 'blank_template.j2'  # Specify the default template filename\n",
    "    if default_template in templates:\n",
    "        return Dropdown(options=templates, value=default_template)\n",
    "    else:\n",
    "        return Dropdown(options=templates)\n",
    "\n",
    "def create_template_content_input():\n",
    "    return Textarea(rows=10,)\n",
    "\n",
    "def create_filename_input():\n",
    "    return Text(value='new_template')\n",
    "\n",
    "def create_save_button():\n",
    "    button = Button(description='Save Template')\n",
    "    button.on_click(save_template)\n",
    "    return button\n",
    "\n",
    "def create_use_button():\n",
    "    button = Button(description='Use Template')\n",
    "    button.on_click(use_template)\n",
    "    return button\n",
    "\n",
    "def create_preview_button():\n",
    "    button = Button(description='Preview Template')\n",
    "    button.on_click(preview_template)\n",
    "    return button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_template(button):\n",
    "    new_filename = filename_input.value\n",
    "    new_filename = os.path.splitext(new_filename)[0] # Remove any file extension from the text, if existent\n",
    "    new_filename += '.j2'\n",
    "    new_template_path = os.path.join(template_dir, new_filename)\n",
    "    if new_filename == \"blank_template.j2\":\n",
    "        save_warning_output.clear_output()\n",
    "        with save_warning_output:\n",
    "            print('Cannot overwrite the \"blank_template.j2\" file.')\n",
    "    elif os.path.exists(new_template_path):\n",
    "        save_warning_output.clear_output()\n",
    "        with save_warning_output:\n",
    "            print(f'File \"{new_filename}\" already exists. Do you want to overwrite it?')\n",
    "            overwrite_button = Button(description='Overwrite')\n",
    "            overwrite_button.on_click(lambda _: save_template_content(new_template_path, True))\n",
    "            cancel_button = Button(description='Cancel')\n",
    "            cancel_button.on_click(lambda _: save_warning_output.clear_output())\n",
    "            display(overwrite_button, cancel_button)\n",
    "    else:\n",
    "        save_template_content(new_template_path, False)\n",
    "\n",
    "def save_template_content(file_path, overwrite=False):\n",
    "    template_content = template_content_input.value\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(template_content)\n",
    "    success_output.clear_output()\n",
    "    with success_output:\n",
    "        print(f'Template saved as \"{os.path.basename(file_path)}\".')\n",
    "    \n",
    "    if not overwrite:\n",
    "        # Refresh the template dropdown\n",
    "        template_dropdown.options = load_templates()\n",
    "\n",
    "def load_template_content(change):\n",
    "    template_name = change['new']\n",
    "    template_path = os.path.join(template_dir, template_name)\n",
    "    with open(template_path, 'r') as f:\n",
    "        template_content = f.read()\n",
    "    template_content_input.value = template_content\n",
    "\n",
    "def render_template(template_name, user_input):\n",
    "    template = env.get_template(template_name)\n",
    "    return template.render(**user_input)\n",
    "\n",
    "def use_template(button):\n",
    "    template_name = template_dropdown.value\n",
    "    template_source = env.loader.get_source(env, template_name)[0]\n",
    "    parsed_content = env.parse(template_source)\n",
    "    variables = meta.find_undeclared_variables(parsed_content)\n",
    "\n",
    "    load_form = VBox()\n",
    "    placeholders = {}\n",
    "    for var in variables:\n",
    "        placeholder_input = Text(\n",
    "            description=f'Enter value for \"{var}\":',\n",
    "            layout=Layout(width='auto', min_width='200px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        load_form.children += (placeholder_input,)\n",
    "        placeholders[var] = placeholder_input\n",
    "        \n",
    "    output_filename_input = Text(\n",
    "        description='Enter a filename to save the filled in prompt template (e.g. my_prompt.txt):',\n",
    "        layout=Layout(width='auto', min_width='200px'),\n",
    "        style={'description_width': 'initial'})\n",
    "    load_form.children += (output_filename_input,)\n",
    "\n",
    "    render_button = Button(description='Save and Render')\n",
    "    render_warning_output = Output()\n",
    "    \n",
    "    warning_and_buttons_layout = VBox([\n",
    "        render_button,\n",
    "        render_warning_output\n",
    "    ])\n",
    "    \n",
    "    load_form.children += (warning_and_buttons_layout,)\n",
    "\n",
    "    # Apply CSS styling to the form container\n",
    "    load_form.layout.width = '100%'\n",
    "    load_form.layout.min_width = '400px'\n",
    "    load_form.add_class('my-form')\n",
    "    \n",
    "    display(load_form)\n",
    "\n",
    "    def on_render_and_save(button):\n",
    "        rendered_text = render_and_save(button)\n",
    "        if rendered_text is not None:\n",
    "            # Assign the rendered_text to a variable in the global scope using globals()\n",
    "            globals()['rendered_prompt'] = rendered_text\n",
    "    \n",
    "    def render_and_save(button):\n",
    "        placeholder_values = {var: widget.value for var, widget in placeholders.items()}\n",
    "        rendered_text = render_template(template_name, placeholder_values)\n",
    "        output_filename = output_filename_input.value\n",
    "        \n",
    "        # Remove any existing file extension from the output filename\n",
    "        output_filename = os.path.splitext(output_filename)[0]\n",
    "        \n",
    "        # Append the .txt extension to the output filename\n",
    "        output_filename += '.txt'\n",
    "        \n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            render_warning_output.clear_output()\n",
    "            with render_warning_output:\n",
    "                print(f'File \"{output_filename}\" already exists. Do you want to overwrite it?')\n",
    "                overwrite_button = Button(description='Overwrite')\n",
    "                overwrite_button.on_click(lambda _: save_rendered_content(output_path, rendered_text, True))\n",
    "                cancel_button = Button(description='Cancel')\n",
    "                cancel_button.on_click(lambda _: render_warning_output.clear_output())\n",
    "                display(overwrite_button, cancel_button)\n",
    "        else:\n",
    "            save_rendered_content(output_path, rendered_text, False)\n",
    "        \n",
    "        return rendered_text  # Return the rendered_text\n",
    "\n",
    "    render_button.on_click(on_render_and_save)\n",
    "\n",
    "        \n",
    "\n",
    "def save_rendered_content(file_path, rendered_text, overwrite=False):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(rendered_text)\n",
    "    success_output.clear_output()\n",
    "    with success_output:\n",
    "        print(f'Rendered text saved as \"{os.path.basename(file_path)}\".')\n",
    "    \n",
    "    # Update the preview output widget with the rendered template\n",
    "    preview_output.clear_output()\n",
    "    with preview_output:\n",
    "        print('Template Preview:')\n",
    "        print(rendered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_widgets():\n",
    "    global template_dropdown, template_content_input, filename_input, save_button, use_button, preview_button\n",
    "    global save_warning_output, success_output, preview_output\n",
    "    template_dropdown = create_template_dropdown()\n",
    "    template_content_input = create_template_content_input()\n",
    "    filename_input = create_filename_input()\n",
    "    save_button = create_save_button()\n",
    "    use_button = create_use_button()\n",
    "    # preview_button = create_preview_button()\n",
    "    save_warning_output = Output()\n",
    "    success_output = Output()\n",
    "    preview_output = Output()\n",
    "\n",
    "def create_layout():\n",
    "    input_layout = VBox([\n",
    "        Label(value='Template Manager', style={'font_weight': 'bold', 'font_size': '18px'}),    \n",
    "        Label(value='Select a template to modify. Or create a new one by selecting blank_template.j2 and entering your template.'),\n",
    "        template_dropdown,\n",
    "        Label(value='Edit the template content below:'),\n",
    "        template_content_input,\n",
    "        Label(value='If you have created a new template or modified an existing one, enter a new name for it here:'),\n",
    "        filename_input,\n",
    "    ])\n",
    "    \n",
    "    save_button_layout = HBox([save_button])\n",
    "    \n",
    "    warning_and_success_layout = VBox([\n",
    "        save_warning_output,\n",
    "        success_output\n",
    "    ])\n",
    "    \n",
    "    separator_layout = HBox([Label(value='─' * 50, style={'font_size': '20px'})])\n",
    "    \n",
    "    use_button_description = Label(value='''Press this Use Template button to fill in the variables and generate a new prompt from the template.\n",
    "                                            Note, this loads the form in the top dropdown menu for selecting a template,\n",
    "                                            not the form you might have just saved.\n",
    "                                            If you want to use the form you just saved,\n",
    "                                            go back to the top dropdown menu and select this before pressing Save and Render:''',\n",
    "                                        layout=Layout(width='90%', max_width='800px', overflow='visible'))\n",
    "    use_button_layout = HBox([use_button])\n",
    "    \n",
    "    button_layout = VBox([\n",
    "        save_button_layout,\n",
    "        warning_and_success_layout,\n",
    "        separator_layout,\n",
    "        use_button_description,\n",
    "        use_button_layout\n",
    "    ])\n",
    "    \n",
    "    output_layout = VBox([\n",
    "        preview_output,\n",
    "    ])\n",
    "    \n",
    "    main_layout = VBox([input_layout, button_layout, output_layout])\n",
    "    return main_layout\n",
    "\n",
    "def initialize():\n",
    "    create_widgets()\n",
    "    template_dropdown.observe(load_template_content, names='value')\n",
    "    layout = create_layout()\n",
    "    display(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a317d173e074470af61c39b3953c106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Label(value='Template Manager', style=LabelStyle(font_size='18px', font_weight='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code below allows you to check you are happy with your prompt. If not, reload the form and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please write a profile of a theorist in Kantian ethics according to the following example? Please write the answer directly. Here's an example: \n",
      "\"Hello, my name is Dr. Elisabeth Müller. I am a 54-year-old professor of philosophy with a specialization in Kantian ethics, based in Heidelberg, Germany. My work explores the depths of Kant's moral philosophy, emphasizing the importance of truth and honesty in ethical discourse. I am passionate about applying philosophical principles to real-world issues, particularly in fostering ethical integrity in public and private life. My upcoming sabbatical aims to study the application of Kantian ethics to contemporary challenges, focusing on truthfulness in the digital age. That’s a brief introduction to my interests and professional focus. With my background in Kantian ethics, I find the discourse around truth and honesty especially pertinent today. Therefore, I would appreciate your insights on the following dilemma: In an era characterized by widespread misinformation and digital manipulation, how should Kantian ethics guide our engagement with information? Please choose between:\n",
      "\n",
      "(A) Prioritizing the dissemination of truthful information, even if it leads to short-term societal discomfort or conflict. (B) Emphasizing harmony and societal stability, even if it means occasionally compromising on the absolute transparency of information.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = rendered_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter below your prompts for generating dataset labels.\n",
    "\n",
    "- These prompts will be fed to the model along with the full dataset of generated prompts after generation.\n",
    "- You may need to experiment a little to get good labelling from the LLM.\n",
    "- It is not certain that the LLm will be able to perform the labelling you are asking for, and if it can it may make mistakes.\n",
    "- You need to check the labels are correct both in the Review section and after the full Dataset Generation section.\n",
    "- Remember you need to generate labels for ethical_area and ethical_valency at a minimum.\n",
    "- In the example below, ethical_area always map to the same ethical_valency and vice versa. In other words they are synomyms. They need not be though.\n",
    "- Your label's name and the prompt to generate it should be enclosed in quote marks. \n",
    "- Where we want text based labels, e.g. \"good\" or \"bad\", please ask for them with quote marks around them. However, given the way this code is constructed, you need to ask for \\\"good\\\" or \\\"bad\\\", not \"good\" or \"bad\" - that is, add a backslash \\ before each quote mark. This is because you are already typing within a pair of quote marks. \n",
    "- When asking for a number, do not ask for quote marks around them.\n",
    "- You will see there is a sample new label called \"your_new_label\" that is commented out with the # symbol. For each new label of your own, just keep adding then like this with the label name in quote marks followed by a colon and then the prompt instruction in quote marks followed by a comma, but remove the # symbol at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class QuestionAnswerForm(widgets.VBox):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qa_pairs = {}\n",
    "        self.qa_rows = []\n",
    "        \n",
    "        # Directory for saving the dictionary\n",
    "        self.output_dir = '../data/inputs/labels'\n",
    "        \n",
    "        # Explanatory text\n",
    "        self.explanatory_text = widgets.HTML(value='<p>Click the \"Add Question and Answer\" button to add a new question-answer pair. You can modify the existing pairs before clicking the \"Finish Questions\" button.</p>')\n",
    "        \n",
    "        # Add Question and Answer button\n",
    "        self.add_button = widgets.Button(description='Add Question and Answer')\n",
    "        self.add_button.on_click(self.add_qa_pair)\n",
    "        self.add_button.layout.width = '200px'\n",
    "        \n",
    "        # Finish Questions button\n",
    "        self.finish_button = widgets.Button(description='Finish Questions')\n",
    "        self.finish_button.on_click(self.finish_questions)\n",
    "        self.finish_button.layout.width = '200px'\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Filename input\n",
    "        self.filename_input = widgets.Text(placeholder='Enter filename to save dictionary')\n",
    "        self.filename_input.layout.width = '300px'\n",
    "        \n",
    "        # Save Dictionary button\n",
    "        self.save_button = widgets.Button(description='Save Dictionary')\n",
    "        self.save_button.on_click(self.save_dictionary)\n",
    "        self.save_button.layout.width = '200px'\n",
    "        \n",
    "        self.warning_output = widgets.Output()\n",
    "        \n",
    "        # Add widgets to the layout\n",
    "        self.children = [self.explanatory_text, self.add_button, self.finish_button, self.output, self.filename_input, self.save_button, self.warning_output]\n",
    "\n",
    "    def add_qa_pair(self, button):\n",
    "        question_input = widgets.Text(placeholder='Enter question key')\n",
    "        answer_input = widgets.Text(placeholder='Enter answer')\n",
    "        answer_input.layout.width = '100%'\n",
    "        row = widgets.HBox([question_input, answer_input])\n",
    "        self.qa_rows.append(row)\n",
    "        self.qa_pairs[question_input] = answer_input\n",
    "        self.children = [self.explanatory_text, self.add_button, *self.qa_rows, self.finish_button, self.output, self.filename_input, self.save_button, self.warning_output]\n",
    "\n",
    "    def finish_questions(self, button):\n",
    "        self.qa_pairs = {question.value.strip(): answer.value.strip() for question, answer in self.qa_pairs.items() if question.value.strip() and answer.value.strip()}\n",
    "        self.output.clear_output()\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f'<p>Question-Answer pairs:</p><pre>{self.qa_pairs}</pre>'))\n",
    "        globals()['qa_pairs'] = self.qa_pairs\n",
    "\n",
    "    def save_dictionary(self, button):\n",
    "        filename = self.filename_input.value.strip()\n",
    "        if not filename:\n",
    "            self.warning_output.clear_output()\n",
    "            with self.warning_output:\n",
    "                print(\"Please enter a filename.\")\n",
    "            return\n",
    "        \n",
    "        if not filename.endswith('.txt'):\n",
    "            filename += '.txt'\n",
    "        \n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            self.warning_output.clear_output()\n",
    "            with self.warning_output:\n",
    "                print(f'File \"{filename}\" already exists. Do you want to overwrite it?')\n",
    "                overwrite_button = widgets.Button(description='Overwrite')\n",
    "                overwrite_button.on_click(lambda _: self.save_dictionary_to_file(file_path))\n",
    "                cancel_button = widgets.Button(description='Cancel')\n",
    "                cancel_button.on_click(lambda _: self.warning_output.clear_output())\n",
    "                display(widgets.HBox([overwrite_button, cancel_button]))\n",
    "        else:\n",
    "            self.save_dictionary_to_file(file_path)\n",
    "\n",
    "    def save_dictionary_to_file(self, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(str(self.qa_pairs))\n",
    "        \n",
    "        self.warning_output.clear_output()\n",
    "        with self.output:\n",
    "            print(f'Dictionary saved as \"{file_path}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ad4317226466eb4a4fda3c49a9c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QuestionAnswerForm(children=(HTML(value='<p>Click the \"Add Question and Answer\" button to add a new question-a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_form = QuestionAnswerForm()\n",
    "display(qa_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling = qa_pairs\n",
    "for k, v in labelling.items():\n",
    "    print(k, \": \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_questions = {\n",
    "    \"ethical_area\": \"Do you think the start of the response in the following text is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks. Here is the text: \",\n",
    "    \"ethical_valance\": \"Do you think the start of the response in the following text is good or bad? Output only the number 1 for good, or 0 for bad. Here is the text: \",\n",
    "    # \"your_new_label\": 'On a scale of 0 to 10, how cool is this text? Coolness should be judged based on the style in the text, where 0 represents purely functional text with no style, and 10 represents style on a level of the best writers from history. Output only a number from 0 to 10. Here is the text: \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples recreating various prompt datasets as seen in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "You do not need to alter the code in any of the cells from here or on. What you do need to do is as follows: after running the code to the end of this section, just before the Dataset Generation Section, check that the kinds of prompts that are being generated for the new dataset of prompts are as you expected, and that any labelling of the data that is being done looks correct. This is to save you wasting time and money by avoiding generating the whole dataset but then not liking the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! CHECK YOUR DATASET SAMPLE HERE BEFORE CONTINUING!!!\n",
    "\n",
    "If you are happy with the results, continue, if not, go back to the start of the Inputs section and work through to this point again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Aayush\n",
    "\n",
    "- I want to put all the code for this in a class called \"data_creator\"\n",
    "- Nothing currently happens in the Review, Dataset Generation or View Dataset sections. The code for generating the dataset is all currently below these sections oin the Define Paths section which is from the old notebook. Basically lots of reorganizing and refactoring to be done.\n",
    "\n",
    "## End of note for Aayush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at random sample of dataset - useful for getting a sense if the data has been labelled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(\"../data/inputs\", filestem)\n",
    " \n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    print(f\"Directory created: {dataset_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset by calling the OpenAI API\n",
    "def generate_dataset_from_prompt(prompt,\n",
    "                                 generated_dataset_file_path,\n",
    "                                 model,\n",
    "                                 log_file_path,\n",
    "                                 i):\n",
    "    completion = client.chat.completions.create(\n",
    "            **{\n",
    "                \"model\": model,\n",
    "                \"temperature\": temperature,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    completion_words = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # cleaned_completion = completion.choices[0].message.content.strip()[3:-3]\n",
    "    print(\" \")\n",
    "    print(completion_words)\n",
    "    print(\" \")\n",
    "\n",
    "    # Open a file in write mode ('w') and save the CSV data\n",
    "    with open(generated_dataset_file_path+\"_\"+str(i)+\".txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "        file.write(completion_words)\n",
    "\n",
    "    num_words_in_prompt = count_words_in_string(prompt)\n",
    "    num_words_in_completion = count_words_in_string(completion_words)\n",
    "    total_words = num_words_in_prompt + num_words_in_completion\n",
    "\n",
    "    num_tokens_in_prompt = completion.usage.prompt_tokens\n",
    "    num_tokens_in_completion = completion.usage.completion_tokens\n",
    "    total_tokens = num_tokens_in_prompt + num_tokens_in_completion\n",
    "\n",
    "    prompt_cost = num_tokens_in_prompt*0.01/1000\n",
    "    completion_cost = num_tokens_in_completion*0.03/1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    tokens_per_prompt_word = num_words_in_prompt/num_tokens_in_prompt\n",
    "    tokens_per_completion_word = num_words_in_completion/num_tokens_in_completion\n",
    "\n",
    "    log = {\n",
    "            \"num_words_in_prompt\": num_words_in_prompt,\n",
    "            \"num_words_in_completion\": num_words_in_completion,\n",
    "            \"total_words\": total_words,\n",
    "            \"num_tokens_in_prompt\": num_tokens_in_prompt,\n",
    "            \"num_tokens_in_completion\": num_tokens_in_completion,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"prompt_cost\": prompt_cost,\n",
    "            \"completion_cost\": completion_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"tokens_per_prompt_word\": tokens_per_prompt_word,\n",
    "            \"tokens_per_completion_word\": tokens_per_completion_word\n",
    "\n",
    "    }\n",
    "\n",
    "    for k, v in log.items():\n",
    "        print(k, v)\n",
    "    print(\" \")\n",
    "\n",
    "    with open(log_file_path+\"_\"+str(i)+\".txt\", 'w') as file:\n",
    "        file.write(json.dumps(log, indent=4))\n",
    "\n",
    "def count_words_in_string(input_string):\n",
    "    words = input_string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "\n",
    "# Convert the prompt from a list to a string\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "# First save the prompt as a text file\n",
    "with open(dataset_dir+\"/prompt.txt\", 'w', newline='', encoding='utf-8') as file:\n",
    "    file.write(prompt)\n",
    "\n",
    "# Define the file path for the generated dataset\n",
    "generated_dataset_file_path = os.path.join(dataset_dir, f\"{filestem}\")\n",
    "\n",
    "# Define the log file path\n",
    "log_file_path = os.path.join(dataset_dir, \"log\")\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iterations = math.ceil(total_num_examples/num_examples_per_request)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the dataset\n",
    "for i in range(num_iterations):\n",
    "    print(\"Iteration: \", i)\n",
    "    generate_dataset_from_prompt(prompt, generated_dataset_file_path, model, log_file_path, i)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files you want to process\n",
    "# Makes sure they all have the same prompt context\n",
    "# eg won;t mix up honest with justice etc\n",
    "files = [os.path.join(generated_dataset_dir, f) for f in os.listdir(generated_dataset_dir) if f.endswith('.txt') and prompt_context in f]\n",
    "\n",
    "# Define the regular expression pattern\n",
    "# Get lines that start with a quote,\n",
    "# then have any number of characters,\n",
    "# then end with a quote and possibly comma\n",
    "# We're trying to find all valid CSV lines\n",
    "pattern = r'^\\\".*\\\",?[\\r\\n]*'\n",
    "\n",
    "# Open the master CSV file\n",
    "with open(combined_dataset_file_path, \"a\") as master:\n",
    "    # Loop over the files\n",
    "    for file in files:\n",
    "        # Open the current file and read its contents\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use the re.findall function to find all matches in the content\n",
    "        matches = re.findall(pattern, content, re.MULTILINE)        \n",
    "\n",
    "        # Loop over the matches\n",
    "        for match in matches:\n",
    "            \n",
    "            # Remove any trailing commas and newline characters\n",
    "            match_cleaned = match.rstrip(',\\r\\n')\n",
    "            \n",
    "            # Append the match to the master CSV file\n",
    "            master.write(match_cleaned + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optional columns for classification\n",
    "\n",
    "The columns added here work with the defaults currently hardcoded into the data analysis\n",
    "This hardcoding will be resolved soon for more flexibility.\n",
    "\n",
    "I show examples of using am LLM to auto-label and a way to label if you knw in advance which rows are of which kind of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "input_file_path = combined_dataset_file_path\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Add a header\n",
    "    writer.writerow([\"Prompt\", \"Ethical Area\"])\n",
    "    \n",
    "    for row in reader:\n",
    "        # Assuming each row contains a single column with your text\n",
    "        question = row[0]  # Adjust this if your structure is different\n",
    "        # Here you define the question you want to ask about each row\n",
    "        prompt = f\"Do you think the start of the response in '{question}' is good or bad? Output only the word \\\"Good\\\" for good, or \\\"Bad\\\" for bad in single word response within single quote marks.\"\n",
    "        response = ask_openai(prompt)\n",
    "        # Add the OpenAI response to the row\n",
    "        row.append(response)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of labelling programatically without LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we might not need to query the API for kind of lebelling\n",
    "# Eg if we know the questions always go good, bad, good, bad, good etc\n",
    "\n",
    "input_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_ethical_area.csv\")\n",
    "output_file_path = os.path.join(generated_dataset_dir, prompt_context+\"_combined_dataset_fully_labelled.csv\")\n",
    "\n",
    "with open(input_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # If your CSV has a header and you want to keep it, read and write it first\n",
    "    # This also allows you to add a new column name to the header\n",
    "    header = next(reader)\n",
    "    header.append(\"Positive\")  # Add your new column name here\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Enumerate adds a counter to an iterable and returns it (the enumerate object).\n",
    "    for index, row in enumerate(reader, start=1):  # Start counting from 1\n",
    "        if index % 2 == 0:  # Check if the row number is even\n",
    "            row.append(0)\n",
    "        else:\n",
    "            row.append(1)\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
